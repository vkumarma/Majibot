{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0dd3cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor (object):\n",
    "    \n",
    "    def __init__(self,data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.autograd = autograd\n",
    "        self.grad = None\n",
    "\n",
    "        if(id is None):\n",
    "            self.id = np.random.randint(0,1000000000)\n",
    "        else:\n",
    "            self.id = id\n",
    "        \n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.children = {}\n",
    "        \n",
    "        if(creators is not None):\n",
    "            for c in creators:\n",
    "                if(self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id,cnt in self.children.items():\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True \n",
    "        \n",
    "    def backward(self,grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    " \n",
    "            if(grad is None):\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "\n",
    "            if(grad_origin is not None):\n",
    "                if(self.children[grad_origin.id] == 0):\n",
    "                    return\n",
    "                    print(self.id)\n",
    "                    print(self.creation_op)\n",
    "                    print(len(self.creators))\n",
    "                    for c in self.creators:\n",
    "                        print(c.creation_op)\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "            if(self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            \n",
    "            # grads must not have grads of their own\n",
    "            assert grad.autograd == False\n",
    "            \n",
    "            # only continue backpropping if there's something to\n",
    "            # backprop into and if all gradients (from children)\n",
    "            # are accounted for override waiting for children if\n",
    "            # \"backprop\" was called on this variable directly\n",
    "            if(self.creators is not None and \n",
    "               (self.all_children_grads_accounted_for() or \n",
    "                grad_origin is None)):\n",
    "\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                    \n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
    "                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
    "\n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new , self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)                    \n",
    "                    \n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    c0 = self.creators[0]\n",
    "                    c1 = self.creators[1]\n",
    "                    new = self.grad.mm(c1.transpose())\n",
    "                    c0.backward(new)\n",
    "                    new = self.grad.transpose().mm(c0).transpose()\n",
    "                    c1.backward(new)\n",
    "                    \n",
    "                if(self.creation_op == \"transpose\"):\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "\n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.expand(dim,\n",
    "                                                               self.creators[0].data.shape[dim]))\n",
    "\n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "                    \n",
    "                if(self.creation_op == \"neg\"):\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                    \n",
    "                if(self.creation_op == \"sigmoid\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
    "                \n",
    "                if(self.creation_op == \"tanh\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "                \n",
    "                if(self.creation_op == \"index_select\"):\n",
    "                    new_grad = np.zeros_like(self.creators[0].data)\n",
    "                    indices_ = self.index_select_indices.data.flatten()\n",
    "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
    "                    for i in range(len(indices_)):\n",
    "                        new_grad[indices_[i]] += grad_[i]\n",
    "                    self.creators[0].backward(Tensor(new_grad))\n",
    "                    \n",
    "                if(self.creation_op == \"cross_entropy\"):\n",
    "                    dx = self.softmax_output - self.target_dist\n",
    "                    self.creators[0].backward(Tensor(dx))\n",
    "                    \n",
    "    def __add__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __neg__(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data * -1,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)    \n",
    "\n",
    "    def sum(self, dim):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sum_\"+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "    \n",
    "    def expand(self, dim,copies):\n",
    "\n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(dim,len(self.data.shape))\n",
    "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
    "        \n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"expand_\"+str(dim))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"transpose\")\n",
    "        \n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self,x],\n",
    "                          creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sigmoid\")\n",
    "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
    "\n",
    "    def tanh(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(np.tanh(self.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"tanh\")\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    def index_select(self, indices):\n",
    "\n",
    "        if(self.autograd):\n",
    "            new = Tensor(self.data[indices.data],\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"index_select\")\n",
    "            new.index_select_indices = indices\n",
    "            return new\n",
    "        return Tensor(self.data[indices.data])\n",
    "    \n",
    "    def softmax(self):\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp,\n",
    "                                       axis=len(self.data.shape)-1,\n",
    "                                       keepdims=True)\n",
    "        return softmax_output\n",
    "    \n",
    "    def cross_entropy(self, target_indices):\n",
    "\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp,\n",
    "                                       axis=len(self.data.shape)-1,\n",
    "                                       keepdims=True)\n",
    "        \n",
    "        t = target_indices.data.flatten()\n",
    "        p = softmax_output.reshape(len(t),-1)\n",
    "        target_dist = np.eye(p.shape[1])[t]\n",
    "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
    "    \n",
    "        if(self.autograd):\n",
    "            out = Tensor(loss,\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"cross_entropy\")\n",
    "            out.softmax_output = softmax_output\n",
    "            out.target_dist = target_dist\n",
    "            return out\n",
    "\n",
    "        return Tensor(loss)\n",
    "        \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n",
    "    \n",
    "class SGD(object): # stochastic gradient descent\n",
    "    def __init__(self, parameters, alpha=0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "    \n",
    "    def step(self, zero=True):\n",
    "        for p in self.parameters:\n",
    "            p.data -= p.grad.data * self.alpha\n",
    "            if (zero):\n",
    "                p.grad.data *= 0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61d0d9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self):\n",
    "        self.parameters = list()\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "    \n",
    "class Linear(Layer):\n",
    "    def __init__(self, n_inputs, n_outputs, bias=True):\n",
    "        super().__init__()\n",
    "        self.use_bias = bias\n",
    "        \n",
    "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))\n",
    "        self.weight = Tensor(W, autograd=True)\n",
    "        \n",
    "        if(self.use_bias):\n",
    "            self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
    "            \n",
    "        self.parameters.append(self.weight)\n",
    "        \n",
    "        if(self.use_bias):        \n",
    "            self.parameters.append(self.bias)\n",
    "\n",
    "        \n",
    "    def forward(self, input):\n",
    "        if(self.use_bias):\n",
    "            return input.mm(self.weight)+self.bias.expand(0,len(input.data))\n",
    "        return input.mm(self.weight)\n",
    "    \n",
    "class Sequential(Layer):\n",
    "    def __init__(self, layers=list()):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        for layer in self.layers: # embed returning (4,3) tensor\n",
    "            input = layer.forward(input)\n",
    "#             print(input, layer)\n",
    "            \n",
    "        return input\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        params = list()\n",
    "        for l in self.layers:\n",
    "            params += l.get_parameters()\n",
    "        return params\n",
    "\n",
    "class MSELoss(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        return ((pred - target) * (pred - target)).sum(0)\n",
    "\n",
    "class CrossEntropyLoss(object):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        return input.cross_entropy(target)\n",
    "    \n",
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()\n",
    "    \n",
    "class Embedding(Layer):\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        \n",
    "        weight = (np.random.rand(vocab_size, dim) - 0.5) / dim\n",
    "        self.weight = Tensor((weight), autograd=True) # weight is tensor\n",
    "        \n",
    "        self.parameters.append(self.weight) # parameters have a weight tensor\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.weight.index_select(input)\n",
    "    \n",
    "class RNNCell(Layer):\n",
    "    def __init__(self, n_inputs, n_hidden, n_output, activation='sigmoid'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        if(activation == \"sigmoid\"):\n",
    "            self.activation = Sigmoid()\n",
    "        elif(activation == \"tanh\"):\n",
    "            self.activation = Tanh()\n",
    "        else:\n",
    "            raise Exception(\"Non-Linearity not found\")\n",
    "        \n",
    "        self.w_ih = Linear(n_inputs, n_hidden)\n",
    "        self.w_hh = Linear(n_hidden, n_hidden)\n",
    "        self.w_ho = Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.parameters += self.w_ih.get_parameters() \n",
    "        self.parameters += self.w_hh.get_parameters()\n",
    "        self.parameters += self.w_ho.get_parameters()\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        from_prev_hidden = self.w_hh.forward(hidden)\n",
    "        combined = self.w_ih.forward(input) + from_prev_hidden \n",
    "        new_hidden = self.activation.forward(combined)\n",
    "        output = self.w_ho.forward(new_hidden)\n",
    "        return output, new_hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return Tensor(np.zeros((batch_size, self.n_hidden)),autograd=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c25b20b",
   "metadata": {},
   "outputs": [],
   "source": [
    " ################################ LSTMs ##########################################\n",
    "class LSTMCell(Layer):\n",
    "    def __init__(self, n_inputs, n_hidden, n_output):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        self.xf = Linear(n_inputs, n_hidden)\n",
    "        self.xi = Linear(n_inputs, n_hidden)\n",
    "        self.xo = Linear(n_inputs, n_hidden)\n",
    "        self.xc = Linear(n_inputs, n_hidden)\n",
    "        self.hf = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.hi = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.ho = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.hc = Linear(n_hidden, n_hidden, bias=False)\n",
    "        \n",
    "        self.w_ho = Linear(n_hidden, n_output, bias=False) # final output layer\n",
    "        \n",
    "        self.parameters += self.xf.get_parameters()\n",
    "        self.parameters += self.xi.get_parameters()\n",
    "        self.parameters += self.xo.get_parameters()\n",
    "        self.parameters += self.xc.get_parameters()\n",
    "        self.parameters += self.hf.get_parameters()\n",
    "        self.parameters += self.hi.get_parameters()\n",
    "        self.parameters += self.ho.get_parameters()\n",
    "        self.parameters += self.hc.get_parameters()\n",
    "        \n",
    "        self.parameters += self.w_ho.get_parameters()\n",
    "        \n",
    "    def forward (self, input, hidden):\n",
    "        prev_hidden = hidden[0] # STM at t-1\n",
    "        prev_cell = hidden[1] # LTM at t-1\n",
    "        \n",
    "        f = (self.xf.forward(input) + self.hf.forward(prev_hidden)).sigmoid() # tensor.sigmoid()\n",
    "        i = (self.xi.forward(input) + self.hi.forward(prev_hidden)).sigmoid()\n",
    "        o = (self.xo.forward(input) + self.ho.forward(prev_hidden)).sigmoid()\n",
    "        g = (self.xc.forward(input) + self.hc.forward(prev_hidden)).tanh()\n",
    "        \n",
    "        c = (f * prev_cell) + (i * g) # c - cell or new LTM at t\n",
    "        h = o * c.tanh() # h - prev_hidden or new STM at t\n",
    "        \n",
    "        output = self.w_ho.forward(h) # output/prediction\n",
    "        return output, (h, c)\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        h = Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)\n",
    "        c = Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)\n",
    "        h.data[:,0] += 1\n",
    "        c.data[:,0] += 1\n",
    "        \n",
    "        return (h,c)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f0f6853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Iter:0 - Alpha:0.05 - Batch 2/249 - Min Loss:62.00 - Loss:62.000013310522844 -          eeeeeeee s s aa yygggwwwwww ww www www ww www ww www www ww w\n",
      " Iter:0 - Alpha:0.05 - Batch 3/249 - Min Loss:61.99 - Loss:61.99112204760545\n",
      " Iter:0 - Alpha:0.05 - Batch 4/249 - Min Loss:61.97 - Loss:61.97525399613958\n",
      " Iter:0 - Alpha:0.05 - Batch 5/249 - Min Loss:61.94 - Loss:61.94633230438216\n",
      " Iter:0 - Alpha:0.05 - Batch 6/249 - Min Loss:61.89 - Loss:61.891202411157884\n",
      " Iter:0 - Alpha:0.05 - Batch 7/249 - Min Loss:61.79 - Loss:61.79890084155188\n",
      " Iter:0 - Alpha:0.05 - Batch 8/249 - Min Loss:61.57 - Loss:61.57942289421901\n",
      " Iter:0 - Alpha:0.05 - Batch 9/249 - Min Loss:61.11 - Loss:61.11618684447217\n",
      " Iter:0 - Alpha:0.05 - Batch 10/249 - Min Loss:60.43 - Loss:60.435814938209\n",
      " Iter:0 - Alpha:0.05 - Batch 11/249 - Min Loss:59.05 - Loss:59.059897746808296\n",
      " Iter:0 - Alpha:0.05 - Batch 12/249 - Min Loss:57.20 - Loss:57.20282045552962\n",
      " Iter:0 - Alpha:0.05 - Batch 13/249 - Min Loss:54.59 - Loss:54.59613675213754\n",
      " Iter:0 - Alpha:0.05 - Batch 14/249 - Min Loss:53.20 - Loss:53.20888497127856\n",
      " Iter:0 - Alpha:0.05 - Batch 15/249 - Min Loss:51.49 - Loss:51.493175314782334\n",
      " Iter:0 - Alpha:0.05 - Batch 16/249 - Min Loss:49.76 - Loss:49.76362295674542\n",
      " Iter:0 - Alpha:0.05 - Batch 17/249 - Min Loss:48.18 - Loss:48.18299931520103\n",
      " Iter:0 - Alpha:0.05 - Batch 18/249 - Min Loss:47.04 - Loss:47.04004030581436\n",
      " Iter:0 - Alpha:0.05 - Batch 19/249 - Min Loss:46.13 - Loss:46.13201069620386\n",
      " Iter:0 - Alpha:0.05 - Batch 20/249 - Min Loss:44.97 - Loss:44.978854835835506\n",
      " Iter:0 - Alpha:0.05 - Batch 21/249 - Min Loss:43.73 - Loss:43.732281136154846\n",
      " Iter:0 - Alpha:0.05 - Batch 22/249 - Min Loss:43.33 - Loss:43.33881231719201\n",
      " Iter:0 - Alpha:0.05 - Batch 23/249 - Min Loss:43.13 - Loss:43.134796535840415\n",
      " Iter:0 - Alpha:0.05 - Batch 24/249 - Min Loss:42.30 - Loss:42.30940384959591\n",
      " Iter:0 - Alpha:0.05 - Batch 25/249 - Min Loss:41.45 - Loss:41.45686216054657\n",
      " Iter:0 - Alpha:0.05 - Batch 26/249 - Min Loss:40.77 - Loss:40.77029236296939\n",
      " Iter:0 - Alpha:0.05 - Batch 27/249 - Min Loss:40.13 - Loss:40.13927326304478\n",
      " Iter:0 - Alpha:0.05 - Batch 28/249 - Min Loss:39.42 - Loss:39.422094824102174\n",
      " Iter:0 - Alpha:0.05 - Batch 29/249 - Min Loss:38.89 - Loss:38.89144977170198\n",
      " Iter:0 - Alpha:0.05 - Batch 30/249 - Min Loss:38.38 - Loss:38.381162860922046\n",
      " Iter:0 - Alpha:0.05 - Batch 31/249 - Min Loss:38.05 - Loss:38.054377495595865\n",
      " Iter:0 - Alpha:0.05 - Batch 32/249 - Min Loss:37.75 - Loss:37.75023466079764\n",
      " Iter:0 - Alpha:0.05 - Batch 33/249 - Min Loss:37.32 - Loss:37.32132997996333\n",
      " Iter:0 - Alpha:0.05 - Batch 34/249 - Min Loss:36.90 - Loss:36.900410129684325\n",
      " Iter:0 - Alpha:0.05 - Batch 35/249 - Min Loss:36.65 - Loss:36.65746644354023\n",
      " Iter:0 - Alpha:0.05 - Batch 36/249 - Min Loss:36.40 - Loss:36.408414777345136\n",
      " Iter:0 - Alpha:0.05 - Batch 37/249 - Min Loss:36.06 - Loss:36.06150487165355\n",
      " Iter:0 - Alpha:0.05 - Batch 38/249 - Min Loss:35.82 - Loss:35.82295790921641\n",
      " Iter:0 - Alpha:0.05 - Batch 39/249 - Min Loss:35.82 - Loss:35.82070358651919\n",
      " Iter:0 - Alpha:0.05 - Batch 40/249 - Min Loss:35.52 - Loss:35.529564165504894\n",
      " Iter:0 - Alpha:0.05 - Batch 41/249 - Min Loss:35.36 - Loss:35.36030560579256\n",
      " Iter:0 - Alpha:0.05 - Batch 42/249 - Min Loss:35.22 - Loss:35.22794949185129\n",
      " Iter:0 - Alpha:0.05 - Batch 43/249 - Min Loss:34.87 - Loss:34.87732259101031\n",
      " Iter:0 - Alpha:0.05 - Batch 44/249 - Min Loss:34.61 - Loss:34.618274862554664\n",
      " Iter:0 - Alpha:0.05 - Batch 45/249 - Min Loss:34.35 - Loss:34.35780585627163\n",
      " Iter:0 - Alpha:0.05 - Batch 46/249 - Min Loss:34.15 - Loss:34.15815620800715\n",
      " Iter:0 - Alpha:0.05 - Batch 47/249 - Min Loss:34.10 - Loss:34.101189632648726\n",
      " Iter:0 - Alpha:0.05 - Batch 48/249 - Min Loss:33.90 - Loss:33.90598627611554\n",
      " Iter:0 - Alpha:0.05 - Batch 49/249 - Min Loss:33.75 - Loss:33.7535673639446\n",
      " Iter:0 - Alpha:0.05 - Batch 50/249 - Min Loss:33.60 - Loss:33.6016508430966\n",
      " Iter:0 - Alpha:0.05 - Batch 51/249 - Min Loss:33.29 - Loss:33.29999991323623\n",
      " Iter:0 - Alpha:0.05 - Batch 52/249 - Min Loss:33.10 - Loss:33.102298482989795\n",
      " Iter:0 - Alpha:0.05 - Batch 53/249 - Min Loss:32.97 - Loss:32.97705706782795\n",
      " Iter:0 - Alpha:0.05 - Batch 54/249 - Min Loss:32.78 - Loss:32.786697652430085\n",
      " Iter:0 - Alpha:0.05 - Batch 55/249 - Min Loss:32.58 - Loss:32.582639716280404\n",
      " Iter:0 - Alpha:0.05 - Batch 56/249 - Min Loss:32.39 - Loss:32.39541412068053\n",
      " Iter:0 - Alpha:0.05 - Batch 57/249 - Min Loss:32.16 - Loss:32.16903899969284\n",
      " Iter:0 - Alpha:0.05 - Batch 58/249 - Min Loss:31.99 - Loss:31.99230392845181\n",
      " Iter:0 - Alpha:0.05 - Batch 59/249 - Min Loss:31.78 - Loss:31.781323166321283\n",
      " Iter:0 - Alpha:0.05 - Batch 60/249 - Min Loss:31.59 - Loss:31.596368959539372\n",
      " Iter:0 - Alpha:0.05 - Batch 61/249 - Min Loss:31.35 - Loss:31.359942308426294\n",
      " Iter:0 - Alpha:0.05 - Batch 62/249 - Min Loss:31.17 - Loss:31.170338512546554\n",
      " Iter:0 - Alpha:0.05 - Batch 63/249 - Min Loss:30.93 - Loss:30.939437158921137\n",
      " Iter:0 - Alpha:0.05 - Batch 64/249 - Min Loss:30.81 - Loss:30.816250232682606\n",
      " Iter:0 - Alpha:0.05 - Batch 66/249 - Min Loss:30.80 - Loss:30.838225795096246\n",
      " Iter:0 - Alpha:0.05 - Batch 67/249 - Min Loss:30.73 - Loss:30.736206447053018\n",
      " Iter:0 - Alpha:0.05 - Batch 68/249 - Min Loss:30.49 - Loss:30.499862619731893\n",
      " Iter:0 - Alpha:0.05 - Batch 69/249 - Min Loss:30.37 - Loss:30.377773750976214\n",
      " Iter:0 - Alpha:0.05 - Batch 70/249 - Min Loss:30.20 - Loss:30.203741441286798\n",
      " Iter:0 - Alpha:0.05 - Batch 71/249 - Min Loss:30.09 - Loss:30.098156897014455\n",
      " Iter:0 - Alpha:0.05 - Batch 72/249 - Min Loss:30.06 - Loss:30.06023654168552\n",
      " Iter:0 - Alpha:0.05 - Batch 73/249 - Min Loss:29.95 - Loss:29.953926987390727\n",
      " Iter:0 - Alpha:0.05 - Batch 74/249 - Min Loss:29.76 - Loss:29.763183517815207\n",
      " Iter:0 - Alpha:0.05 - Batch 75/249 - Min Loss:29.62 - Loss:29.620851801690076\n",
      " Iter:0 - Alpha:0.05 - Batch 76/249 - Min Loss:29.51 - Loss:29.511923861733994\n",
      " Iter:0 - Alpha:0.05 - Batch 77/249 - Min Loss:29.36 - Loss:29.369623378825544\n",
      " Iter:0 - Alpha:0.05 - Batch 78/249 - Min Loss:29.29 - Loss:29.298924994069676\n",
      " Iter:0 - Alpha:0.05 - Batch 79/249 - Min Loss:29.17 - Loss:29.172021162199872\n",
      " Iter:0 - Alpha:0.05 - Batch 80/249 - Min Loss:29.04 - Loss:29.046999498416994\n",
      " Iter:0 - Alpha:0.05 - Batch 81/249 - Min Loss:28.88 - Loss:28.88522290213245\n",
      " Iter:0 - Alpha:0.05 - Batch 82/249 - Min Loss:28.80 - Loss:28.80073192571315\n",
      " Iter:0 - Alpha:0.05 - Batch 86/249 - Min Loss:28.74 - Loss:28.868883524538468\n",
      " Iter:0 - Alpha:0.05 - Batch 87/249 - Min Loss:28.69 - Loss:28.69854548028015\n",
      " Iter:0 - Alpha:0.05 - Batch 88/249 - Min Loss:28.54 - Loss:28.54072144337157\n",
      " Iter:0 - Alpha:0.05 - Batch 89/249 - Min Loss:28.38 - Loss:28.380020978770197\n",
      " Iter:0 - Alpha:0.05 - Batch 90/249 - Min Loss:28.23 - Loss:28.23532199914782\n",
      " Iter:0 - Alpha:0.05 - Batch 91/249 - Min Loss:28.10 - Loss:28.102989159663913\n",
      " Iter:0 - Alpha:0.05 - Batch 92/249 - Min Loss:27.99 - Loss:27.999292311715706\n",
      " Iter:0 - Alpha:0.05 - Batch 93/249 - Min Loss:27.85 - Loss:27.855609138109227\n",
      " Iter:0 - Alpha:0.05 - Batch 94/249 - Min Loss:27.72 - Loss:27.726474476686956\n",
      " Iter:0 - Alpha:0.05 - Batch 95/249 - Min Loss:27.58 - Loss:27.58449489184428\n",
      " Iter:0 - Alpha:0.05 - Batch 96/249 - Min Loss:27.49 - Loss:27.497847315199557\n",
      " Iter:0 - Alpha:0.05 - Batch 97/249 - Min Loss:27.37 - Loss:27.379318599926645\n",
      " Iter:0 - Alpha:0.05 - Batch 98/249 - Min Loss:27.26 - Loss:27.265078754469446\n",
      " Iter:0 - Alpha:0.05 - Batch 99/249 - Min Loss:27.09 - Loss:27.099457927002632\n",
      " Iter:0 - Alpha:0.05 - Batch 100/249 - Min Loss:26.95 - Loss:26.957717840315805\n",
      " Iter:0 - Alpha:0.05 - Batch 101/249 - Min Loss:26.81 - Loss:26.813229431941696\n",
      " Iter:0 - Alpha:0.05 - Batch 102/249 - Min Loss:26.68 - Loss:26.684212246773434\n",
      " Iter:0 - Alpha:0.05 - Batch 103/249 - Min Loss:26.57 - Loss:26.57163523739692\n",
      " Iter:0 - Alpha:0.05 - Batch 104/249 - Min Loss:26.50 - Loss:26.500842371063033\n",
      " Iter:0 - Alpha:0.05 - Batch 105/249 - Min Loss:26.48 - Loss:26.480471046211516\n",
      " Iter:0 - Alpha:0.05 - Batch 106/249 - Min Loss:26.36 - Loss:26.36517131048776\n",
      " Iter:0 - Alpha:0.05 - Batch 107/249 - Min Loss:26.29 - Loss:26.297023090704364\n",
      " Iter:0 - Alpha:0.05 - Batch 108/249 - Min Loss:26.19 - Loss:26.19442794716367\n",
      " Iter:0 - Alpha:0.05 - Batch 109/249 - Min Loss:26.11 - Loss:26.110015490215766\n",
      " Iter:0 - Alpha:0.05 - Batch 110/249 - Min Loss:26.01 - Loss:26.0150252801616\n",
      " Iter:0 - Alpha:0.05 - Batch 111/249 - Min Loss:25.90 - Loss:25.902133707479216\n",
      " Iter:0 - Alpha:0.05 - Batch 112/249 - Min Loss:25.80 - Loss:25.80672067837178\n",
      " Iter:0 - Alpha:0.05 - Batch 113/249 - Min Loss:25.72 - Loss:25.72431476723884\n",
      " Iter:0 - Alpha:0.05 - Batch 114/249 - Min Loss:25.66 - Loss:25.66320916232698\n",
      " Iter:0 - Alpha:0.05 - Batch 115/249 - Min Loss:25.63 - Loss:25.63650616802204\n",
      " Iter:0 - Alpha:0.05 - Batch 116/249 - Min Loss:25.56 - Loss:25.564784397647472\n",
      " Iter:0 - Alpha:0.05 - Batch 117/249 - Min Loss:25.48 - Loss:25.485782208313683\n",
      " Iter:0 - Alpha:0.05 - Batch 118/249 - Min Loss:25.38 - Loss:25.385591867399228\n",
      " Iter:0 - Alpha:0.05 - Batch 119/249 - Min Loss:25.31 - Loss:25.31406082504817\n",
      " Iter:0 - Alpha:0.05 - Batch 120/249 - Min Loss:25.21 - Loss:25.21708212867403\n",
      " Iter:0 - Alpha:0.05 - Batch 121/249 - Min Loss:25.12 - Loss:25.126168992763866\n",
      " Iter:0 - Alpha:0.05 - Batch 122/249 - Min Loss:25.02 - Loss:25.025932348444343\n",
      " Iter:0 - Alpha:0.05 - Batch 123/249 - Min Loss:24.92 - Loss:24.926529298548328\n",
      " Iter:0 - Alpha:0.05 - Batch 124/249 - Min Loss:24.83 - Loss:24.83637616099367\n",
      " Iter:0 - Alpha:0.05 - Batch 125/249 - Min Loss:24.75 - Loss:24.756749278134198\n",
      " Iter:0 - Alpha:0.05 - Batch 126/249 - Min Loss:24.68 - Loss:24.6875523766737\n",
      " Iter:0 - Alpha:0.05 - Batch 127/249 - Min Loss:24.60 - Loss:24.6024682967009\n",
      " Iter:0 - Alpha:0.05 - Batch 128/249 - Min Loss:24.49 - Loss:24.493146870517865\n",
      " Iter:0 - Alpha:0.05 - Batch 129/249 - Min Loss:24.39 - Loss:24.397361959147645\n",
      " Iter:0 - Alpha:0.05 - Batch 130/249 - Min Loss:24.32 - Loss:24.325811354902655\n",
      " Iter:0 - Alpha:0.05 - Batch 131/249 - Min Loss:24.23 - Loss:24.231305681846266\n",
      " Iter:0 - Alpha:0.05 - Batch 132/249 - Min Loss:24.13 - Loss:24.13351720451929\n",
      " Iter:0 - Alpha:0.05 - Batch 133/249 - Min Loss:24.04 - Loss:24.045605006211492\n",
      " Iter:0 - Alpha:0.05 - Batch 134/249 - Min Loss:23.95 - Loss:23.952848762554808\n",
      " Iter:0 - Alpha:0.05 - Batch 135/249 - Min Loss:23.89 - Loss:23.896564378638963\n",
      " Iter:0 - Alpha:0.05 - Batch 136/249 - Min Loss:23.86 - Loss:23.869348654717562\n",
      " Iter:0 - Alpha:0.05 - Batch 137/249 - Min Loss:23.80 - Loss:23.800759748395585\n",
      " Iter:0 - Alpha:0.05 - Batch 138/249 - Min Loss:23.72 - Loss:23.725524941718028\n",
      " Iter:0 - Alpha:0.05 - Batch 140/249 - Min Loss:23.71 - Loss:23.725141157618864\n",
      " Iter:0 - Alpha:0.05 - Batch 141/249 - Min Loss:23.65 - Loss:23.658555742577242\n",
      " Iter:0 - Alpha:0.05 - Batch 142/249 - Min Loss:23.57 - Loss:23.571014007612263\n",
      " Iter:0 - Alpha:0.05 - Batch 143/249 - Min Loss:23.48 - Loss:23.48611963892898\n",
      " Iter:0 - Alpha:0.05 - Batch 144/249 - Min Loss:23.39 - Loss:23.396855199020006\n",
      " Iter:0 - Alpha:0.05 - Batch 145/249 - Min Loss:23.32 - Loss:23.326283863128854\n",
      " Iter:0 - Alpha:0.05 - Batch 146/249 - Min Loss:23.30 - Loss:23.308910716891187\n",
      " Iter:0 - Alpha:0.05 - Batch 147/249 - Min Loss:23.25 - Loss:23.252917089418936\n",
      " Iter:0 - Alpha:0.05 - Batch 148/249 - Min Loss:23.19 - Loss:23.198039297093615\n",
      " Iter:0 - Alpha:0.05 - Batch 149/249 - Min Loss:23.12 - Loss:23.12676876969974\n",
      " Iter:0 - Alpha:0.05 - Batch 150/249 - Min Loss:23.06 - Loss:23.06885724391895\n",
      " Iter:0 - Alpha:0.05 - Batch 151/249 - Min Loss:23.02 - Loss:23.025996923051306\n",
      " Iter:0 - Alpha:0.05 - Batch 152/249 - Min Loss:22.96 - Loss:22.965171042931946\n",
      " Iter:0 - Alpha:0.05 - Batch 153/249 - Min Loss:22.87 - Loss:22.877370358251742\n",
      " Iter:0 - Alpha:0.05 - Batch 154/249 - Min Loss:22.80 - Loss:22.803760464606462\n",
      " Iter:0 - Alpha:0.05 - Batch 155/249 - Min Loss:22.76 - Loss:22.76540856981397\n",
      " Iter:0 - Alpha:0.05 - Batch 156/249 - Min Loss:22.74 - Loss:22.746789048570548\n",
      " Iter:0 - Alpha:0.05 - Batch 157/249 - Min Loss:22.70 - Loss:22.702170300365264\n",
      " Iter:0 - Alpha:0.05 - Batch 158/249 - Min Loss:22.64 - Loss:22.643797478828883\n",
      " Iter:0 - Alpha:0.05 - Batch 159/249 - Min Loss:22.60 - Loss:22.607723519181153\n",
      " Iter:0 - Alpha:0.05 - Batch 160/249 - Min Loss:22.54 - Loss:22.546510115053227\n",
      " Iter:0 - Alpha:0.05 - Batch 161/249 - Min Loss:22.47 - Loss:22.47907056575688\n",
      " Iter:0 - Alpha:0.05 - Batch 162/249 - Min Loss:22.42 - Loss:22.420308425796136\n",
      " Iter:0 - Alpha:0.05 - Batch 163/249 - Min Loss:22.36 - Loss:22.361139893562036\n",
      " Iter:0 - Alpha:0.05 - Batch 164/249 - Min Loss:22.31 - Loss:22.314313947664743\n",
      " Iter:0 - Alpha:0.05 - Batch 165/249 - Min Loss:22.27 - Loss:22.27812274848683\n",
      " Iter:0 - Alpha:0.05 - Batch 166/249 - Min Loss:22.22 - Loss:22.226140535857347\n",
      " Iter:0 - Alpha:0.05 - Batch 167/249 - Min Loss:22.20 - Loss:22.202262683628085\n",
      " Iter:0 - Alpha:0.05 - Batch 168/249 - Min Loss:22.17 - Loss:22.1749852261495\n",
      " Iter:0 - Alpha:0.05 - Batch 169/249 - Min Loss:22.11 - Loss:22.119315334733017\n",
      " Iter:0 - Alpha:0.05 - Batch 170/249 - Min Loss:22.07 - Loss:22.072905967267246\n",
      " Iter:0 - Alpha:0.05 - Batch 171/249 - Min Loss:22.03 - Loss:22.039360747751694\n",
      " Iter:0 - Alpha:0.05 - Batch 172/249 - Min Loss:22.00 - Loss:22.00685407036866\n",
      " Iter:0 - Alpha:0.05 - Batch 173/249 - Min Loss:21.97 - Loss:21.971336635269736\n",
      " Iter:0 - Alpha:0.05 - Batch 174/249 - Min Loss:21.93 - Loss:21.931308654645342\n",
      " Iter:0 - Alpha:0.05 - Batch 175/249 - Min Loss:21.89 - Loss:21.899790645114717\n",
      " Iter:0 - Alpha:0.05 - Batch 176/249 - Min Loss:21.85 - Loss:21.856012772163197\n",
      " Iter:0 - Alpha:0.05 - Batch 177/249 - Min Loss:21.80 - Loss:21.806483666176845\n",
      " Iter:0 - Alpha:0.05 - Batch 178/249 - Min Loss:21.75 - Loss:21.759531095043613\n",
      " Iter:0 - Alpha:0.05 - Batch 179/249 - Min Loss:21.73 - Loss:21.737805350537684\n",
      " Iter:0 - Alpha:0.05 - Batch 180/249 - Min Loss:21.69 - Loss:21.69172127334886\n",
      " Iter:0 - Alpha:0.05 - Batch 181/249 - Min Loss:21.64 - Loss:21.64175669980318\n",
      " Iter:0 - Alpha:0.05 - Batch 182/249 - Min Loss:21.59 - Loss:21.591485035972198\n",
      " Iter:0 - Alpha:0.05 - Batch 183/249 - Min Loss:21.53 - Loss:21.537518844132414\n",
      " Iter:0 - Alpha:0.05 - Batch 184/249 - Min Loss:21.49 - Loss:21.49710451707273\n",
      " Iter:0 - Alpha:0.05 - Batch 185/249 - Min Loss:21.45 - Loss:21.453760109346714\n",
      " Iter:0 - Alpha:0.05 - Batch 186/249 - Min Loss:21.42 - Loss:21.424049287610355\n",
      " Iter:0 - Alpha:0.05 - Batch 187/249 - Min Loss:21.39 - Loss:21.39540988880556\n",
      " Iter:0 - Alpha:0.05 - Batch 188/249 - Min Loss:21.38 - Loss:21.388878098169148\n",
      " Iter:0 - Alpha:0.05 - Batch 189/249 - Min Loss:21.35 - Loss:21.351594679741712\n",
      " Iter:0 - Alpha:0.05 - Batch 190/249 - Min Loss:21.32 - Loss:21.322804773215406\n",
      " Iter:0 - Alpha:0.05 - Batch 191/249 - Min Loss:21.28 - Loss:21.289536110368225\n",
      " Iter:0 - Alpha:0.05 - Batch 192/249 - Min Loss:21.25 - Loss:21.25877807354337\n",
      " Iter:0 - Alpha:0.05 - Batch 193/249 - Min Loss:21.19 - Loss:21.195497117652877\n",
      " Iter:0 - Alpha:0.05 - Batch 194/249 - Min Loss:21.17 - Loss:21.175539804815763\n",
      " Iter:0 - Alpha:0.05 - Batch 195/249 - Min Loss:21.16 - Loss:21.168347429977977\n",
      " Iter:0 - Alpha:0.05 - Batch 196/249 - Min Loss:21.12 - Loss:21.122133002206247\n",
      " Iter:0 - Alpha:0.05 - Batch 197/249 - Min Loss:21.09 - Loss:21.09057996910744\n",
      " Iter:0 - Alpha:0.05 - Batch 198/249 - Min Loss:21.05 - Loss:21.059946366825418\n",
      " Iter:0 - Alpha:0.05 - Batch 199/249 - Min Loss:21.01 - Loss:21.01591482653068\n",
      " Iter:0 - Alpha:0.05 - Batch 200/249 - Min Loss:20.96 - Loss:20.9651114915892\n",
      " Iter:0 - Alpha:0.05 - Batch 201/249 - Min Loss:20.93 - Loss:20.933152338690597\n",
      " Iter:0 - Alpha:0.05 - Batch 202/249 - Min Loss:20.90 - Loss:20.904299311620182\n",
      " Iter:0 - Alpha:0.05 - Batch 203/249 - Min Loss:20.86 - Loss:20.866424752588046\n",
      " Iter:0 - Alpha:0.05 - Batch 204/249 - Min Loss:20.82 - Loss:20.829031591387352\n",
      " Iter:0 - Alpha:0.05 - Batch 205/249 - Min Loss:20.79 - Loss:20.795269390467716\n",
      " Iter:0 - Alpha:0.05 - Batch 206/249 - Min Loss:20.75 - Loss:20.75110963213048\n",
      " Iter:0 - Alpha:0.05 - Batch 207/249 - Min Loss:20.71 - Loss:20.712090145021364\n",
      " Iter:0 - Alpha:0.05 - Batch 208/249 - Min Loss:20.67 - Loss:20.673139194462916\n",
      " Iter:0 - Alpha:0.05 - Batch 209/249 - Min Loss:20.62 - Loss:20.624422050823895\n",
      " Iter:0 - Alpha:0.05 - Batch 210/249 - Min Loss:20.57 - Loss:20.577138398562965\n",
      " Iter:0 - Alpha:0.05 - Batch 211/249 - Min Loss:20.52 - Loss:20.52111836378247\n",
      " Iter:0 - Alpha:0.05 - Batch 212/249 - Min Loss:20.47 - Loss:20.474293819369727\n",
      " Iter:0 - Alpha:0.05 - Batch 213/249 - Min Loss:20.43 - Loss:20.438193961431303\n",
      " Iter:0 - Alpha:0.05 - Batch 214/249 - Min Loss:20.40 - Loss:20.40250946330838\n",
      " Iter:0 - Alpha:0.05 - Batch 215/249 - Min Loss:20.35 - Loss:20.355626184077387\n",
      " Iter:0 - Alpha:0.05 - Batch 216/249 - Min Loss:20.31 - Loss:20.313860946521046\n",
      " Iter:0 - Alpha:0.05 - Batch 217/249 - Min Loss:20.28 - Loss:20.285423099147614\n",
      " Iter:0 - Alpha:0.05 - Batch 218/249 - Min Loss:20.24 - Loss:20.24016149006698\n",
      " Iter:0 - Alpha:0.05 - Batch 219/249 - Min Loss:20.18 - Loss:20.18540178059378\n",
      " Iter:0 - Alpha:0.05 - Batch 220/249 - Min Loss:20.15 - Loss:20.159343883306416\n",
      " Iter:0 - Alpha:0.05 - Batch 221/249 - Min Loss:20.14 - Loss:20.14892114694874\n",
      " Iter:0 - Alpha:0.05 - Batch 222/249 - Min Loss:20.11 - Loss:20.1144911155793\n",
      " Iter:0 - Alpha:0.05 - Batch 223/249 - Min Loss:20.07 - Loss:20.0794092950445\n",
      " Iter:0 - Alpha:0.05 - Batch 224/249 - Min Loss:20.04 - Loss:20.046992324057285\n",
      " Iter:0 - Alpha:0.05 - Batch 225/249 - Min Loss:20.01 - Loss:20.012129412405713\n",
      " Iter:0 - Alpha:0.05 - Batch 226/249 - Min Loss:19.97 - Loss:19.972458494961423\n",
      " Iter:0 - Alpha:0.05 - Batch 227/249 - Min Loss:19.94 - Loss:19.942620744791608\n",
      " Iter:0 - Alpha:0.05 - Batch 228/249 - Min Loss:19.92 - Loss:19.927641972906798\n",
      " Iter:0 - Alpha:0.05 - Batch 229/249 - Min Loss:19.89 - Loss:19.895511309931337\n",
      " Iter:0 - Alpha:0.05 - Batch 230/249 - Min Loss:19.86 - Loss:19.86524367349002\n",
      " Iter:0 - Alpha:0.05 - Batch 231/249 - Min Loss:19.84 - Loss:19.845738469129547\n",
      " Iter:0 - Alpha:0.05 - Batch 232/249 - Min Loss:19.81 - Loss:19.81138939825413\n",
      " Iter:0 - Alpha:0.05 - Batch 233/249 - Min Loss:19.79 - Loss:19.790881324640935\n",
      " Iter:0 - Alpha:0.05 - Batch 234/249 - Min Loss:19.75 - Loss:19.758060184954527\n",
      " Iter:0 - Alpha:0.05 - Batch 235/249 - Min Loss:19.72 - Loss:19.721510076187148\n",
      " Iter:0 - Alpha:0.05 - Batch 236/249 - Min Loss:19.68 - Loss:19.68445543544956\n",
      " Iter:0 - Alpha:0.05 - Batch 237/249 - Min Loss:19.65 - Loss:19.65836971391111\n",
      " Iter:0 - Alpha:0.05 - Batch 238/249 - Min Loss:19.63 - Loss:19.638671838341804\n",
      " Iter:0 - Alpha:0.05 - Batch 239/249 - Min Loss:19.61 - Loss:19.615280460443987\n",
      " Iter:0 - Alpha:0.05 - Batch 240/249 - Min Loss:19.58 - Loss:19.588933249084374\n",
      " Iter:0 - Alpha:0.05 - Batch 241/249 - Min Loss:19.54 - Loss:19.549199426219506\n",
      " Iter:0 - Alpha:0.05 - Batch 242/249 - Min Loss:19.51 - Loss:19.51759842236851\n",
      " Iter:0 - Alpha:0.05 - Batch 244/249 - Min Loss:19.48 - Loss:19.490534566513933\n",
      " Iter:0 - Alpha:0.05 - Batch 245/249 - Min Loss:19.46 - Loss:19.467577658827885\n",
      " Iter:0 - Alpha:0.05 - Batch 246/249 - Min Loss:19.44 - Loss:19.44190740345133\n",
      " Iter:0 - Alpha:0.05 - Batch 247/249 - Min Loss:19.41 - Loss:19.416108458445333\n",
      " Iter:0 - Alpha:0.05 - Batch 248/249 - Min Loss:19.38 - Loss:19.38745286955047\n",
      " Iter:0 - Alpha:0.05 - Batch 249/249 - Min Loss:19.36 - Loss:19.360154365977415\n",
      " Iter:1 - Alpha:0.049 - Batch 1/249 - Min Loss:12.99 - Loss:12.996803987832056 - he ther t tere t tere t tere t tere t tere t tere t tere t tere t tere\n",
      " Iter:1 - Alpha:0.049 - Batch 3/249 - Min Loss:12.89 - Loss:13.006624322892922\n",
      " Iter:1 - Alpha:0.049 - Batch 4/249 - Min Loss:12.89 - Loss:12.89244417362653\n",
      " Iter:2 - Alpha:0.049 - Batch 4/249 - Min Loss:12.88 - Loss:13.178723464676345- hendendendendendendendendendendendendendendendendendendendendendendend\n",
      " Iter:2 - Alpha:0.049 - Batch 28/249 - Min Loss:12.80 - Loss:12.834542403324003\n",
      " Iter:2 - Alpha:0.049 - Batch 29/249 - Min Loss:12.80 - Loss:12.803790290604692\n",
      " Iter:2 - Alpha:0.049 - Batch 30/249 - Min Loss:12.80 - Loss:12.803181353427831\n",
      " Iter:2 - Alpha:0.049 - Batch 31/249 - Min Loss:12.76 - Loss:12.768711432980616\n",
      " Iter:2 - Alpha:0.049 - Batch 33/249 - Min Loss:12.73 - Loss:12.743073133124668\n",
      " Iter:2 - Alpha:0.049 - Batch 34/249 - Min Loss:12.72 - Loss:12.721801600811725\n",
      " Iter:2 - Alpha:0.049 - Batch 35/249 - Min Loss:12.67 - Loss:12.675702526334156\n",
      " Iter:2 - Alpha:0.049 - Batch 56/249 - Min Loss:12.65 - Loss:12.660419407422301\n",
      " Iter:2 - Alpha:0.049 - Batch 57/249 - Min Loss:12.63 - Loss:12.635588981170038\n",
      " Iter:2 - Alpha:0.049 - Batch 60/249 - Min Loss:12.63 - Loss:12.649034467563135\n",
      " Iter:2 - Alpha:0.049 - Batch 96/249 - Min Loss:12.62 - Loss:12.625456443031403\n",
      " Iter:2 - Alpha:0.049 - Batch 97/249 - Min Loss:12.61 - Loss:12.614168074375728\n",
      " Iter:2 - Alpha:0.049 - Batch 98/249 - Min Loss:12.60 - Loss:12.600206606472597\n",
      " Iter:2 - Alpha:0.049 - Batch 99/249 - Min Loss:12.59 - Loss:12.59894281322269\n",
      " Iter:2 - Alpha:0.049 - Batch 100/249 - Min Loss:12.59 - Loss:12.59078289729839\n",
      " Iter:2 - Alpha:0.049 - Batch 203/249 - Min Loss:12.58 - Loss:12.592894520486821\n",
      " Iter:2 - Alpha:0.049 - Batch 204/249 - Min Loss:12.58 - Loss:12.58724474819104\n",
      " Iter:2 - Alpha:0.049 - Batch 205/249 - Min Loss:12.58 - Loss:12.586838824901289\n",
      " Iter:2 - Alpha:0.049 - Batch 206/249 - Min Loss:12.58 - Loss:12.580996722160766\n",
      " Iter:2 - Alpha:0.049 - Batch 207/249 - Min Loss:12.57 - Loss:12.572575575325367\n",
      " Iter:2 - Alpha:0.049 - Batch 208/249 - Min Loss:12.56 - Loss:12.566059768824624\n",
      " Iter:2 - Alpha:0.049 - Batch 209/249 - Min Loss:12.55 - Loss:12.556579770730414\n",
      " Iter:2 - Alpha:0.049 - Batch 210/249 - Min Loss:12.54 - Loss:12.546965609467621\n",
      " Iter:2 - Alpha:0.049 - Batch 211/249 - Min Loss:12.52 - Loss:12.529455460799968\n",
      " Iter:2 - Alpha:0.049 - Batch 212/249 - Min Loss:12.52 - Loss:12.521251724833055\n",
      " Iter:2 - Alpha:0.049 - Batch 213/249 - Min Loss:12.51 - Loss:12.51492172664\n",
      " Iter:2 - Alpha:0.049 - Batch 214/249 - Min Loss:12.51 - Loss:12.511417637608446\n",
      " Iter:2 - Alpha:0.049 - Batch 215/249 - Min Loss:12.50 - Loss:12.500694022546485\n",
      " Iter:2 - Alpha:0.049 - Batch 217/249 - Min Loss:12.49 - Loss:12.494215665465484\n",
      " Iter:2 - Alpha:0.049 - Batch 218/249 - Min Loss:12.48 - Loss:12.486596910450828\n",
      " Iter:2 - Alpha:0.049 - Batch 219/249 - Min Loss:12.47 - Loss:12.471832029045217\n",
      " Iter:2 - Alpha:0.049 - Batch 235/249 - Min Loss:12.46 - Loss:12.472418946866327\n",
      " Iter:2 - Alpha:0.049 - Batch 249/249 - Min Loss:12.46 - Loss:12.477651680977694\n",
      " Iter:3 - Alpha:0.048 - Batch 3/249 - Min Loss:12.42 - Loss:12.569570856951358 - hend theneseres, Thend theneseres, Thend theneseres, Thend theneseres,\n",
      " Iter:3 - Alpha:0.048 - Batch 4/249 - Min Loss:12.41 - Loss:12.414414513741729\n",
      " Iter:3 - Alpha:0.048 - Batch 103/249 - Min Loss:12.21 - Loss:12.210907908907371\n",
      " Iter:3 - Alpha:0.048 - Batch 104/249 - Min Loss:12.20 - Loss:12.208355625944769\n",
      " Iter:3 - Alpha:0.048 - Batch 105/249 - Min Loss:12.20 - Loss:12.20712955732916\n",
      " Iter:3 - Alpha:0.048 - Batch 107/249 - Min Loss:12.20 - Loss:12.208992008898644\n",
      " Iter:3 - Alpha:0.048 - Batch 108/249 - Min Loss:12.19 - Loss:12.197814050899856\n",
      " Iter:3 - Alpha:0.048 - Batch 119/249 - Min Loss:12.19 - Loss:12.202423936347406\n",
      " Iter:3 - Alpha:0.048 - Batch 120/249 - Min Loss:12.18 - Loss:12.187833701076672\n",
      " Iter:3 - Alpha:0.048 - Batch 121/249 - Min Loss:12.18 - Loss:12.180768129947852\n",
      " Iter:3 - Alpha:0.048 - Batch 130/249 - Min Loss:12.17 - Loss:12.178214807308294\n",
      " Iter:3 - Alpha:0.048 - Batch 131/249 - Min Loss:12.15 - Loss:12.155874533295405\n",
      " Iter:3 - Alpha:0.048 - Batch 133/249 - Min Loss:12.13 - Loss:12.142629546009495\n",
      " Iter:3 - Alpha:0.048 - Batch 135/249 - Min Loss:12.13 - Loss:12.142772918385024\n",
      " Iter:3 - Alpha:0.048 - Batch 137/249 - Min Loss:12.13 - Loss:12.133555256945837\n",
      " Iter:3 - Alpha:0.048 - Batch 201/249 - Min Loss:12.12 - Loss:12.124959256157211\n",
      " Iter:3 - Alpha:0.048 - Batch 203/249 - Min Loss:12.12 - Loss:12.120712505495023\n",
      " Iter:3 - Alpha:0.048 - Batch 205/249 - Min Loss:12.11 - Loss:12.115437208990304\n",
      " Iter:3 - Alpha:0.048 - Batch 206/249 - Min Loss:12.11 - Loss:12.110053204450697\n",
      " Iter:3 - Alpha:0.048 - Batch 207/249 - Min Loss:12.10 - Loss:12.10293892839727\n",
      " Iter:3 - Alpha:0.048 - Batch 208/249 - Min Loss:12.09 - Loss:12.095843828461861\n",
      " Iter:3 - Alpha:0.048 - Batch 209/249 - Min Loss:12.08 - Loss:12.086795260881233\n",
      " Iter:3 - Alpha:0.048 - Batch 210/249 - Min Loss:12.07 - Loss:12.076974329973682\n",
      " Iter:3 - Alpha:0.048 - Batch 211/249 - Min Loss:12.05 - Loss:12.057628860031041\n",
      " Iter:3 - Alpha:0.048 - Batch 212/249 - Min Loss:12.04 - Loss:12.049014246026196\n",
      " Iter:3 - Alpha:0.048 - Batch 213/249 - Min Loss:12.04 - Loss:12.04337718305157\n",
      " Iter:3 - Alpha:0.048 - Batch 214/249 - Min Loss:12.03 - Loss:12.039539158967848\n",
      " Iter:3 - Alpha:0.048 - Batch 215/249 - Min Loss:12.02 - Loss:12.027989928855128\n",
      " Iter:3 - Alpha:0.048 - Batch 217/249 - Min Loss:12.01 - Loss:12.019715922682307\n",
      " Iter:3 - Alpha:0.048 - Batch 218/249 - Min Loss:12.01 - Loss:12.011449608535745\n",
      " Iter:4 - Alpha:0.048 - Batch 16/249 - Min Loss:11.99 - Loss:12.002886491045842- hend, Thend, Thend and seates,  Beat, Thend and seates,  Beat, Thend a\n",
      " Iter:4 - Alpha:0.048 - Batch 17/249 - Min Loss:11.98 - Loss:11.986019928281861\n",
      " Iter:4 - Alpha:0.048 - Batch 18/249 - Min Loss:11.91 - Loss:11.913513946039455\n",
      " Iter:4 - Alpha:0.048 - Batch 23/249 - Min Loss:11.88 - Loss:11.944784855821926\n",
      " Iter:4 - Alpha:0.048 - Batch 24/249 - Min Loss:11.86 - Loss:11.865105036123857\n",
      " Iter:4 - Alpha:0.048 - Batch 25/249 - Min Loss:11.77 - Loss:11.771412462479544\n",
      " Iter:4 - Alpha:0.048 - Batch 34/249 - Min Loss:11.66 - Loss:11.678188876119318\n",
      " Iter:4 - Alpha:0.048 - Batch 35/249 - Min Loss:11.64 - Loss:11.64418932735016\n",
      " Iter:4 - Alpha:0.048 - Batch 55/249 - Min Loss:11.63 - Loss:11.648469696279264\n",
      " Iter:4 - Alpha:0.048 - Batch 56/249 - Min Loss:11.61 - Loss:11.617002699058789\n",
      " Iter:4 - Alpha:0.048 - Batch 100/249 - Min Loss:11.60 - Loss:11.606127528834115\n",
      " Iter:4 - Alpha:0.048 - Batch 101/249 - Min Loss:11.59 - Loss:11.59324461966825\n",
      " Iter:4 - Alpha:0.048 - Batch 102/249 - Min Loss:11.57 - Loss:11.576075118382338\n",
      " Iter:4 - Alpha:0.048 - Batch 103/249 - Min Loss:11.57 - Loss:11.57066952240574\n",
      " Iter:4 - Alpha:0.048 - Batch 104/249 - Min Loss:11.56 - Loss:11.568329155113704\n",
      " Iter:4 - Alpha:0.048 - Batch 105/249 - Min Loss:11.56 - Loss:11.568101923803312\n",
      " Iter:4 - Alpha:0.048 - Batch 106/249 - Min Loss:11.55 - Loss:11.552524400397406\n",
      " Iter:4 - Alpha:0.048 - Batch 107/249 - Min Loss:11.55 - Loss:11.550213258000431\n",
      " Iter:4 - Alpha:0.048 - Batch 126/249 - Min Loss:11.54 - Loss:11.545491477675013\n",
      " Iter:4 - Alpha:0.048 - Batch 127/249 - Min Loss:11.53 - Loss:11.535028531447095\n",
      " Iter:4 - Alpha:0.048 - Batch 128/249 - Min Loss:11.53 - Loss:11.534458032337529\n",
      " Iter:4 - Alpha:0.048 - Batch 129/249 - Min Loss:11.52 - Loss:11.525701102631446\n",
      " Iter:4 - Alpha:0.048 - Batch 130/249 - Min Loss:11.52 - Loss:11.521331322115367\n",
      " Iter:4 - Alpha:0.048 - Batch 131/249 - Min Loss:11.50 - Loss:11.504401157161714\n",
      " Iter:4 - Alpha:0.048 - Batch 133/249 - Min Loss:11.48 - Loss:11.491577809134604\n",
      " Iter:4 - Alpha:0.048 - Batch 135/249 - Min Loss:11.48 - Loss:11.48531825739327\n",
      " Iter:4 - Alpha:0.048 - Batch 137/249 - Min Loss:11.47 - Loss:11.478195197760542\n",
      " Iter:4 - Alpha:0.048 - Batch 152/249 - Min Loss:11.46 - Loss:11.470892466591737\n",
      " Iter:4 - Alpha:0.048 - Batch 153/249 - Min Loss:11.46 - Loss:11.461954257135428\n",
      " Iter:4 - Alpha:0.048 - Batch 209/249 - Min Loss:11.45 - Loss:11.458451691996141\n",
      " Iter:4 - Alpha:0.048 - Batch 210/249 - Min Loss:11.45 - Loss:11.45049893589888\n",
      " Iter:4 - Alpha:0.048 - Batch 211/249 - Min Loss:11.43 - Loss:11.430900397356508\n",
      " Iter:4 - Alpha:0.048 - Batch 212/249 - Min Loss:11.42 - Loss:11.4238177004693\n",
      " Iter:4 - Alpha:0.048 - Batch 213/249 - Min Loss:11.41 - Loss:11.418547856853923\n",
      " Iter:4 - Alpha:0.048 - Batch 214/249 - Min Loss:11.41 - Loss:11.415437086928451\n",
      " Iter:4 - Alpha:0.048 - Batch 215/249 - Min Loss:11.40 - Loss:11.403902297029854\n",
      " Iter:4 - Alpha:0.048 - Batch 217/249 - Min Loss:11.39 - Loss:11.398858252408285\n",
      " Iter:4 - Alpha:0.048 - Batch 218/249 - Min Loss:11.39 - Loss:11.390608416869991\n",
      " Iter:4 - Alpha:0.048 - Batch 226/249 - Min Loss:11.37 - Loss:11.378608315832798\n",
      " Iter:4 - Alpha:0.048 - Batch 249/249 - Min Loss:11.37 - Loss:11.408361103914926\n",
      " Iter:5 - Alpha:0.047 - Batch 1/249 - Min Loss:11.31 - Loss:11.319890101077043 - hend and seat theseres, and seateseres, and seateseres, and seateseres\n",
      " Iter:5 - Alpha:0.047 - Batch 2/249 - Min Loss:11.27 - Loss:11.279518792058985\n",
      " Iter:5 - Alpha:0.047 - Batch 4/249 - Min Loss:11.21 - Loss:11.285400020534478\n",
      " Iter:5 - Alpha:0.047 - Batch 37/249 - Min Loss:11.19 - Loss:11.206063405144295\n",
      " Iter:5 - Alpha:0.047 - Batch 55/249 - Min Loss:11.18 - Loss:11.210896469475243\n",
      " Iter:5 - Alpha:0.047 - Batch 56/249 - Min Loss:11.17 - Loss:11.179594415685807\n",
      " Iter:5 - Alpha:0.047 - Batch 97/249 - Min Loss:11.15 - Loss:11.161052581613584\n",
      " Iter:5 - Alpha:0.047 - Batch 99/249 - Min Loss:11.14 - Loss:11.150487809836157\n",
      " Iter:5 - Alpha:0.047 - Batch 100/249 - Min Loss:11.14 - Loss:11.141628147922956\n",
      " Iter:5 - Alpha:0.047 - Batch 101/249 - Min Loss:11.12 - Loss:11.125224949192846\n",
      " Iter:5 - Alpha:0.047 - Batch 102/249 - Min Loss:11.10 - Loss:11.1062241434858\n",
      " Iter:5 - Alpha:0.047 - Batch 103/249 - Min Loss:11.10 - Loss:11.101407171917126\n",
      " Iter:5 - Alpha:0.047 - Batch 104/249 - Min Loss:11.09 - Loss:11.096966405662926\n",
      " Iter:5 - Alpha:0.047 - Batch 105/249 - Min Loss:11.09 - Loss:11.094753845808599\n",
      " Iter:5 - Alpha:0.047 - Batch 106/249 - Min Loss:11.07 - Loss:11.079781875183539\n",
      " Iter:5 - Alpha:0.047 - Batch 107/249 - Min Loss:11.07 - Loss:11.07687155749368\n",
      " Iter:5 - Alpha:0.047 - Batch 129/249 - Min Loss:11.07 - Loss:11.074077664104555\n",
      " Iter:5 - Alpha:0.047 - Batch 130/249 - Min Loss:11.07 - Loss:11.070580093983699\n",
      " Iter:5 - Alpha:0.047 - Batch 131/249 - Min Loss:11.05 - Loss:11.05536238832964\n",
      " Iter:5 - Alpha:0.047 - Batch 133/249 - Min Loss:11.03 - Loss:11.043931908729953\n",
      " Iter:5 - Alpha:0.047 - Batch 135/249 - Min Loss:11.03 - Loss:11.035187113326785\n",
      " Iter:5 - Alpha:0.047 - Batch 137/249 - Min Loss:11.02 - Loss:11.026051879701308\n",
      " Iter:5 - Alpha:0.047 - Batch 138/249 - Min Loss:11.01 - Loss:11.011346796277762\n",
      " Iter:5 - Alpha:0.047 - Batch 143/249 - Min Loss:11.00 - Loss:11.015285095444806\n",
      " Iter:5 - Alpha:0.047 - Batch 144/249 - Min Loss:11.00 - Loss:11.004387055648282\n",
      " Iter:5 - Alpha:0.047 - Batch 150/249 - Min Loss:10.99 - Loss:11.003369233109456\n",
      " Iter:5 - Alpha:0.047 - Batch 151/249 - Min Loss:10.99 - Loss:10.999174580357655\n",
      " Iter:5 - Alpha:0.047 - Batch 152/249 - Min Loss:10.99 - Loss:10.993737520225821\n",
      " Iter:5 - Alpha:0.047 - Batch 153/249 - Min Loss:10.98 - Loss:10.98727052771799\n",
      " Iter:5 - Alpha:0.047 - Batch 208/249 - Min Loss:10.97 - Loss:10.985897692348955\n",
      " Iter:5 - Alpha:0.047 - Batch 209/249 - Min Loss:10.97 - Loss:10.978322789407065\n",
      " Iter:5 - Alpha:0.047 - Batch 210/249 - Min Loss:10.97 - Loss:10.970663405853765\n",
      " Iter:5 - Alpha:0.047 - Batch 211/249 - Min Loss:10.95 - Loss:10.950891435226572\n",
      " Iter:5 - Alpha:0.047 - Batch 212/249 - Min Loss:10.94 - Loss:10.945133668587388\n",
      " Iter:5 - Alpha:0.047 - Batch 213/249 - Min Loss:10.93 - Loss:10.938376435138856\n",
      " Iter:5 - Alpha:0.047 - Batch 214/249 - Min Loss:10.93 - Loss:10.93620058733061\n",
      " Iter:5 - Alpha:0.047 - Batch 215/249 - Min Loss:10.92 - Loss:10.925449361189592\n",
      " Iter:5 - Alpha:0.047 - Batch 217/249 - Min Loss:10.91 - Loss:10.922181953959398\n",
      " Iter:5 - Alpha:0.047 - Batch 218/249 - Min Loss:10.91 - Loss:10.914004408417538\n",
      " Iter:5 - Alpha:0.047 - Batch 222/249 - Min Loss:10.90 - Loss:10.904360475065689\n",
      " Iter:5 - Alpha:0.047 - Batch 223/249 - Min Loss:10.90 - Loss:10.900591992456006\n",
      " Iter:5 - Alpha:0.047 - Batch 224/249 - Min Loss:10.89 - Loss:10.89817810982655\n",
      " Iter:5 - Alpha:0.047 - Batch 225/249 - Min Loss:10.89 - Loss:10.894489906890115\n",
      " Iter:5 - Alpha:0.047 - Batch 226/249 - Min Loss:10.89 - Loss:10.89118583978066\n",
      " Iter:5 - Alpha:0.047 - Batch 227/249 - Min Loss:10.88 - Loss:10.885281391446552\n",
      " Iter:5 - Alpha:0.047 - Batch 235/249 - Min Loss:10.88 - Loss:10.884296443384708\n",
      " Iter:5 - Alpha:0.047 - Batch 236/249 - Min Loss:10.87 - Loss:10.875901767210499\n",
      " Iter:5 - Alpha:0.047 - Batch 237/249 - Min Loss:10.86 - Loss:10.869291034841439\n",
      " Iter:6 - Alpha:0.047 - Batch 55/249 - Min Loss:10.86 - Loss:10.886674166547092- hend and seates, Buthes, and seates, Buthes, and seates, Buthes, and s\n",
      " Iter:6 - Alpha:0.047 - Batch 56/249 - Min Loss:10.85 - Loss:10.858993250171508\n",
      " Iter:6 - Alpha:0.047 - Batch 99/249 - Min Loss:10.84 - Loss:10.843041275467389\n",
      " Iter:6 - Alpha:0.047 - Batch 100/249 - Min Loss:10.83 - Loss:10.833624894788459\n",
      " Iter:6 - Alpha:0.047 - Batch 101/249 - Min Loss:10.81 - Loss:10.816844570926058\n",
      " Iter:6 - Alpha:0.047 - Batch 102/249 - Min Loss:10.79 - Loss:10.798450947594128\n",
      " Iter:6 - Alpha:0.047 - Batch 103/249 - Min Loss:10.79 - Loss:10.797278858616032\n",
      " Iter:6 - Alpha:0.047 - Batch 104/249 - Min Loss:10.79 - Loss:10.792840687303618\n",
      " Iter:6 - Alpha:0.047 - Batch 105/249 - Min Loss:10.79 - Loss:10.790112763674587\n",
      " Iter:6 - Alpha:0.047 - Batch 106/249 - Min Loss:10.77 - Loss:10.776374842773746\n",
      " Iter:6 - Alpha:0.047 - Batch 107/249 - Min Loss:10.77 - Loss:10.772579042074502\n",
      " Iter:6 - Alpha:0.047 - Batch 130/249 - Min Loss:10.77 - Loss:10.771146388923283\n",
      " Iter:6 - Alpha:0.047 - Batch 131/249 - Min Loss:10.75 - Loss:10.758397462853774\n",
      " Iter:6 - Alpha:0.047 - Batch 133/249 - Min Loss:10.74 - Loss:10.747122708706479\n",
      " Iter:6 - Alpha:0.047 - Batch 135/249 - Min Loss:10.74 - Loss:10.743177967422534\n",
      " Iter:6 - Alpha:0.047 - Batch 137/249 - Min Loss:10.73 - Loss:10.738336453969346\n",
      " Iter:6 - Alpha:0.047 - Batch 152/249 - Min Loss:10.73 - Loss:10.730454006722056\n",
      " Iter:6 - Alpha:0.047 - Batch 153/249 - Min Loss:10.72 - Loss:10.72608369351772\n",
      " Iter:6 - Alpha:0.047 - Batch 209/249 - Min Loss:10.72 - Loss:10.723975472348625\n",
      " Iter:6 - Alpha:0.047 - Batch 210/249 - Min Loss:10.71 - Loss:10.715273508462577\n",
      " Iter:6 - Alpha:0.047 - Batch 211/249 - Min Loss:10.69 - Loss:10.695868530099043\n",
      " Iter:6 - Alpha:0.047 - Batch 212/249 - Min Loss:10.68 - Loss:10.689935055385726\n",
      " Iter:6 - Alpha:0.047 - Batch 213/249 - Min Loss:10.68 - Loss:10.68242731038715\n",
      " Iter:6 - Alpha:0.047 - Batch 214/249 - Min Loss:10.68 - Loss:10.68110795273817\n",
      " Iter:6 - Alpha:0.047 - Batch 215/249 - Min Loss:10.67 - Loss:10.671532945204348\n",
      " Iter:6 - Alpha:0.047 - Batch 217/249 - Min Loss:10.66 - Loss:10.668940025388569\n",
      " Iter:6 - Alpha:0.047 - Batch 218/249 - Min Loss:10.66 - Loss:10.661265402923888\n",
      " Iter:6 - Alpha:0.047 - Batch 219/249 - Min Loss:10.64 - Loss:10.64802089672578\n",
      " Iter:6 - Alpha:0.047 - Batch 221/249 - Min Loss:10.64 - Loss:10.651399118554641\n",
      " Iter:6 - Alpha:0.047 - Batch 222/249 - Min Loss:10.64 - Loss:10.646504971111744\n",
      " Iter:6 - Alpha:0.047 - Batch 223/249 - Min Loss:10.64 - Loss:10.64206109019037\n",
      " Iter:6 - Alpha:0.047 - Batch 224/249 - Min Loss:10.63 - Loss:10.63936620173612\n",
      " Iter:6 - Alpha:0.047 - Batch 225/249 - Min Loss:10.63 - Loss:10.63547116741557\n",
      " Iter:6 - Alpha:0.047 - Batch 226/249 - Min Loss:10.63 - Loss:10.631168446955034\n",
      " Iter:6 - Alpha:0.047 - Batch 227/249 - Min Loss:10.62 - Loss:10.625336009188892\n",
      " Iter:6 - Alpha:0.047 - Batch 233/249 - Min Loss:10.62 - Loss:10.627528779055414\n",
      " Iter:6 - Alpha:0.047 - Batch 234/249 - Min Loss:10.61 - Loss:10.61972094164563\n",
      " Iter:6 - Alpha:0.047 - Batch 235/249 - Min Loss:10.61 - Loss:10.612040260818674\n",
      " Iter:6 - Alpha:0.047 - Batch 236/249 - Min Loss:10.60 - Loss:10.603764605964814\n",
      " Iter:6 - Alpha:0.047 - Batch 237/249 - Min Loss:10.59 - Loss:10.597459409012258\n",
      " Iter:7 - Alpha:0.046 - Batch 127/249 - Min Loss:10.59 - Loss:10.602754043584419 hen theseen theseen theseen theseen theseen theseen theseen theseen th\n",
      " Iter:7 - Alpha:0.046 - Batch 128/249 - Min Loss:10.59 - Loss:10.591225828924815\n",
      " Iter:7 - Alpha:0.046 - Batch 129/249 - Min Loss:10.58 - Loss:10.58447382275333\n",
      " Iter:7 - Alpha:0.046 - Batch 130/249 - Min Loss:10.57 - Loss:10.579158340862973\n",
      " Iter:7 - Alpha:0.046 - Batch 131/249 - Min Loss:10.56 - Loss:10.567064142527768\n",
      " Iter:7 - Alpha:0.046 - Batch 132/249 - Min Loss:10.54 - Loss:10.549909450747306\n",
      " Iter:7 - Alpha:0.046 - Batch 133/249 - Min Loss:10.54 - Loss:10.547237697801426\n",
      " Iter:7 - Alpha:0.046 - Batch 135/249 - Min Loss:10.53 - Loss:10.533709980634056\n",
      " Iter:7 - Alpha:0.046 - Batch 137/249 - Min Loss:10.52 - Loss:10.527064451036818\n",
      " Iter:7 - Alpha:0.046 - Batch 138/249 - Min Loss:10.51 - Loss:10.515895769253952\n",
      " Iter:7 - Alpha:0.046 - Batch 210/249 - Min Loss:10.51 - Loss:10.522421937001694\n",
      " Iter:7 - Alpha:0.046 - Batch 211/249 - Min Loss:10.50 - Loss:10.503277546870127\n",
      " Iter:7 - Alpha:0.046 - Batch 212/249 - Min Loss:10.49 - Loss:10.498122913794427\n",
      " Iter:7 - Alpha:0.046 - Batch 213/249 - Min Loss:10.49 - Loss:10.491054969029586\n",
      " Iter:7 - Alpha:0.046 - Batch 214/249 - Min Loss:10.49 - Loss:10.490327620024976\n",
      " Iter:7 - Alpha:0.046 - Batch 215/249 - Min Loss:10.48 - Loss:10.48193275914618\n",
      " Iter:7 - Alpha:0.046 - Batch 217/249 - Min Loss:10.47 - Loss:10.479953326877819\n",
      " Iter:7 - Alpha:0.046 - Batch 218/249 - Min Loss:10.47 - Loss:10.472433223832297\n",
      " Iter:7 - Alpha:0.046 - Batch 219/249 - Min Loss:10.45 - Loss:10.459307586925533\n",
      " Iter:7 - Alpha:0.046 - Batch 221/249 - Min Loss:10.45 - Loss:10.461470441560403\n",
      " Iter:7 - Alpha:0.046 - Batch 222/249 - Min Loss:10.45 - Loss:10.455874400816674\n",
      " Iter:7 - Alpha:0.046 - Batch 223/249 - Min Loss:10.45 - Loss:10.450975506848016\n",
      " Iter:7 - Alpha:0.046 - Batch 224/249 - Min Loss:10.44 - Loss:10.448588600899647\n",
      " Iter:7 - Alpha:0.046 - Batch 225/249 - Min Loss:10.44 - Loss:10.444491139393854\n",
      " Iter:7 - Alpha:0.046 - Batch 226/249 - Min Loss:10.44 - Loss:10.440436316307185\n",
      " Iter:7 - Alpha:0.046 - Batch 227/249 - Min Loss:10.43 - Loss:10.435180260579319\n",
      " Iter:7 - Alpha:0.046 - Batch 228/249 - Min Loss:10.43 - Loss:10.43348356161255\n",
      " Iter:7 - Alpha:0.046 - Batch 230/249 - Min Loss:10.43 - Loss:10.433011259725038\n",
      " Iter:7 - Alpha:0.046 - Batch 231/249 - Min Loss:10.43 - Loss:10.431067559691558\n",
      " Iter:7 - Alpha:0.046 - Batch 233/249 - Min Loss:10.43 - Loss:10.434741335675378\n",
      " Iter:7 - Alpha:0.046 - Batch 234/249 - Min Loss:10.42 - Loss:10.427312096282598\n",
      " Iter:7 - Alpha:0.046 - Batch 235/249 - Min Loss:10.41 - Loss:10.419358256860072\n",
      " Iter:7 - Alpha:0.046 - Batch 236/249 - Min Loss:10.41 - Loss:10.411533107224395\n",
      " Iter:8 - Alpha:0.046 - Batch 132/249 - Min Loss:10.40 - Loss:10.409787134074072 hen theen theen theen theen theen theen theen theen theen theen theen \n",
      " Iter:8 - Alpha:0.046 - Batch 133/249 - Min Loss:10.40 - Loss:10.406018902590601\n",
      " Iter:8 - Alpha:0.046 - Batch 135/249 - Min Loss:10.38 - Loss:10.390261017593385\n",
      " Iter:8 - Alpha:0.046 - Batch 137/249 - Min Loss:10.38 - Loss:10.382769951919551\n",
      " Iter:8 - Alpha:0.046 - Batch 138/249 - Min Loss:10.37 - Loss:10.370503393205645\n",
      " Iter:8 - Alpha:0.046 - Batch 143/249 - Min Loss:10.36 - Loss:10.366715031219803\n",
      " Iter:8 - Alpha:0.046 - Batch 144/249 - Min Loss:10.36 - Loss:10.362460845201905\n",
      " Iter:8 - Alpha:0.046 - Batch 152/249 - Min Loss:10.36 - Loss:10.366522060081717\n",
      " Iter:8 - Alpha:0.046 - Batch 153/249 - Min Loss:10.36 - Loss:10.360778275825865\n",
      " Iter:8 - Alpha:0.046 - Batch 154/249 - Min Loss:10.35 - Loss:10.35511124183572\n",
      " Iter:8 - Alpha:0.046 - Batch 208/249 - Min Loss:10.35 - Loss:10.361253168478155\n",
      " Iter:8 - Alpha:0.046 - Batch 209/249 - Min Loss:10.35 - Loss:10.353432614542795\n",
      " Iter:8 - Alpha:0.046 - Batch 210/249 - Min Loss:10.34 - Loss:10.347167724416316\n",
      " Iter:8 - Alpha:0.046 - Batch 211/249 - Min Loss:10.32 - Loss:10.328540357435132\n",
      " Iter:8 - Alpha:0.046 - Batch 212/249 - Min Loss:10.32 - Loss:10.323025888930758\n",
      " Iter:8 - Alpha:0.046 - Batch 213/249 - Min Loss:10.31 - Loss:10.316472645309043\n",
      " Iter:8 - Alpha:0.046 - Batch 214/249 - Min Loss:10.31 - Loss:10.316465042759143\n",
      " Iter:8 - Alpha:0.046 - Batch 215/249 - Min Loss:10.30 - Loss:10.307926891939635\n",
      " Iter:8 - Alpha:0.046 - Batch 217/249 - Min Loss:10.30 - Loss:10.304111364733872\n",
      " Iter:8 - Alpha:0.046 - Batch 218/249 - Min Loss:10.29 - Loss:10.296607872267153\n",
      " Iter:8 - Alpha:0.046 - Batch 221/249 - Min Loss:10.28 - Loss:10.288280590545348\n",
      " Iter:8 - Alpha:0.046 - Batch 222/249 - Min Loss:10.28 - Loss:10.28220004294702\n",
      " Iter:8 - Alpha:0.046 - Batch 223/249 - Min Loss:10.27 - Loss:10.277110176923689\n",
      " Iter:8 - Alpha:0.046 - Batch 224/249 - Min Loss:10.27 - Loss:10.273540160996836\n",
      " Iter:8 - Alpha:0.046 - Batch 225/249 - Min Loss:10.26 - Loss:10.269350223288367\n",
      " Iter:8 - Alpha:0.046 - Batch 226/249 - Min Loss:10.26 - Loss:10.26647250340978\n",
      " Iter:8 - Alpha:0.046 - Batch 231/249 - Min Loss:10.26 - Loss:10.262253111574697\n",
      " Iter:8 - Alpha:0.046 - Batch 233/249 - Min Loss:10.26 - Loss:10.267617072190733\n",
      " Iter:8 - Alpha:0.046 - Batch 234/249 - Min Loss:10.26 - Loss:10.260832294623983\n",
      " Iter:8 - Alpha:0.046 - Batch 235/249 - Min Loss:10.25 - Loss:10.254359861336336\n",
      " Iter:8 - Alpha:0.046 - Batch 236/249 - Min Loss:10.24 - Loss:10.247261119923095\n",
      " Iter:8 - Alpha:0.046 - Batch 237/249 - Min Loss:10.24 - Loss:10.241289590282832\n",
      " Iter:8 - Alpha:0.046 - Batch 249/249 - Min Loss:10.23 - Loss:10.248437340580018\n",
      " Iter:49 - Alpha:0.030 - Batch 249/249 - Min Loss:10.02 - Loss:15.534770044802679he me knest the seathe dead, wer memy them memake the my head, and beet"
     ]
    }
   ],
   "source": [
    "import sys, random, math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "f = open('/Users/vivekkumarmaheshwari/Downloads/Grokking-Deep-Learning-master/shakespear.txt')\n",
    "raw = f.read()\n",
    "f.close()\n",
    "\n",
    "# print(raw[0:500])\n",
    "# raw is a big block of string. Therefore, is iterable\n",
    "\n",
    "vocab = list(set(raw)) # list of unique characters in the dataset\n",
    "word2index = {} # making sure that every character has a index\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "\n",
    "\n",
    "y = lambda x:word2index[x] # y is function with one argument x, and returns index for a character\n",
    "temp = list(map(y,raw)) # function y is applied to string - raw\n",
    "# print(temp)\n",
    "indices = np.array(temp) # raw - string is converted into list of indices representing characters\n",
    "#######################################################################\n",
    "embed = Embedding(vocab_size=len(vocab), dim=512)\n",
    "model = LSTMCell (n_inputs=512, n_hidden=512,n_output=len(vocab))\n",
    "model.w_ho.weight.data *= 0\n",
    "\n",
    "criterion = CrossEntropyLoss() # loss function \n",
    "optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)\n",
    "\n",
    "batch_size = 16\n",
    "bptt = 25 # back propogation truncated timestep\n",
    "n_batches = int((indices.shape[0]) / (batch_size)) # 3124 in this case\n",
    "\n",
    "trimmed_indices = indices[:n_batches*batch_size] # dataset an even multiple between batch_size and n_batches\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches) # (32, 3124)\n",
    "batched_indices = batched_indices.transpose() # (3124, 32)\n",
    "# for i in batched_indices:\n",
    "#     print(i)\n",
    "\n",
    "input_batched_indices = batched_indices[0:-1]\n",
    "target_batched_indices = batched_indices[1:] \n",
    "# target is input with a offset of one row, so that the network predicts the next character.\n",
    "\n",
    "n_bptt = int(((n_batches-1) / bptt)) # number of small datasets 195\n",
    "input_batches = input_batched_indices[:n_bptt*bptt] # [:3120]\n",
    "input_batches = input_batches.reshape(n_bptt, bptt, batch_size) # (195, 16, 32) \n",
    "# 195 => rows or small datasets, bptt or columns => 16, batch_size or tuple of 32\n",
    "  \n",
    "target_batches = target_batched_indices[:n_bptt*bptt]\n",
    "target_batches = target_batches.reshape(n_bptt,bptt, batch_size)\n",
    "\n",
    "def train(iterations=100):\n",
    "    min_loss = 1000\n",
    "    for iter in range(iterations):\n",
    "        total_loss = 0\n",
    "        n_loss = 0\n",
    "        \n",
    "        hidden = model.init_hidden(batch_size=batch_size)\n",
    "        batches_to_train = len(input_batches)\n",
    "        \n",
    "        for batch_i in range(batches_to_train): # 0 -> 194\n",
    "            \n",
    "            hidden = (Tensor(hidden[0].data, autograd=True), Tensor(hidden[1].data, autograd=True))\n",
    "#             loss = None\n",
    "            losses = list()\n",
    "            \n",
    "            for t in range(bptt): # 0 -> 15, # at every time step batch of 32 characters processed\n",
    "                input = Tensor(input_batches[batch_i][t], autograd=True) # (1,32)\n",
    "                rnn_input = embed.forward(input=input) # (32,512)\n",
    "                output, hidden = model.forward(input=rnn_input,hidden=hidden) \n",
    "                # output => (32,62), hidden => (32, 512)\n",
    "                \n",
    "                target = Tensor(target_batches[batch_i][t], autograd=True)\n",
    "                batch_loss = criterion.forward(output, target)\n",
    "                \n",
    "                if(t == 0):\n",
    "                    losses.append(batch_loss)\n",
    "                else:\n",
    "                    losses.append(batch_loss + losses[-1])\n",
    "                    \n",
    "            loss = losses[-1] \n",
    "            \n",
    "            loss.backward() # backpropogation\n",
    "            optim.step() # stochastic gradient descent\n",
    "            \n",
    "            total_loss += loss.data / bptt\n",
    "            \n",
    "            epoch_loss = np.exp(total_loss / (batch_i+1))\n",
    "            if(epoch_loss < min_loss):\n",
    "                min_loss = epoch_loss\n",
    "                print()\n",
    "\n",
    "            log = \"\\r Iter:\" + str(iter)\n",
    "            log += \" - Alpha:\" + str(optim.alpha)[0:5]\n",
    "            log += \" - Batch \"+str(batch_i+1)+\"/\"+str(len(input_batches))\n",
    "            log += \" - Min Loss:\" + str(min_loss)[0:5]\n",
    "            log += \" - Loss:\" + str(epoch_loss)\n",
    "            if(batch_i == 0):\n",
    "                log += \" - \" + generate_sample(n=70, init_char='T').replace(\"\\n\",\" \")\n",
    "            if(batch_i % 1 == 0):\n",
    "                sys.stdout.write(log)\n",
    "                \n",
    "        optim.alpha *= 0.99\n",
    "        \n",
    "train(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37cc2d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1abbc5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All my lord, trusine.\n",
      "\n",
      "ANTI will do be rink will d be rind bak lord, will  our gull dor Jo se to love our the lo se would brie illove our for ave and will powill not will dor in think will mo will paclor of will do illlove ber\n",
      "All my will do-ll Ford, will prouin suncornd in thaid will dor Ond  a will dor oull pover thiness, age will d berind bak lord, will sill mor oundin to will d by my be dord, if will pot\n",
      "All sill not will do be rind  in in the llow of cloman clord, will sill more thou shoull pot\n",
      "Ho lord, tid will sin, in thak of cdoul  of will do\n",
      "I O clllor groud io shoull pant will porive ation sime of will do\n",
      "I O will do\n",
      "I O will prove will not will do be rind bak will do berillows:\n",
      "All my lord, to will sill my, wir do will do\n",
      "More tll my dore to will dor cll my my lord, thiness,\n",
      "All my paclore the would b owill dor in tharing  will prove will not will dor and will pould ring you will derivill in, will do e lord the world se will d be re to see to love of Ror:\n",
      "All my lord, truing you will do e llowse will lord e will not will do be rind brie k of will prouid Tillowios of will lor to ou my will do-ll For thentespell dor in sime of will do\n",
      "More to llow of thentleentanclllo'd will proud of thy pain spart!\n",
      "\n",
      "All my partam, will dor cll my my lord, think wind bried in e will derio will do be id llow of tit pose wollow, nor clome o sill sing, and will pould ring you will derios suppll my,\n",
      "Wirs upoll my will sill mor ound bill to ou loved brie will dorld injt your trink will do be rin thin thine e of clord know of cllord in the know of clll pose will my,\n",
      "And will sill mor ounding powlous love be barn,\n",
      "Willow, will not swe world biok of will do\n",
      "I O will do be if love our ould bre the for and will dor and will pore thand will dsir clll do e will not spe will lord, if lovery, indull my will d Co will prove will not will do be rind bak will do berillows:\n",
      "All my lord, to whould bre the for and will dor or\n",
      "Mowoull derrior Fnoull pose will lord, if lovery, indull my will d D\n"
     ]
    }
   ],
   "source": [
    "def generate_sample(n=30, init_char=' '):\n",
    "    s = \"\"\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    input = Tensor(np.array([word2index[init_char]]))\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "        output.data *= 15\n",
    "        temp_dist = output.softmax()\n",
    "        temp_dist /= temp_dist.sum()\n",
    "        \n",
    "        m = output.data.argmax() # take the max prediction\n",
    "        c = vocab[m]\n",
    "        input = Tensor(np.array([m]))\n",
    "        s += c\n",
    "    return s\n",
    "print(generate_sample(n=2000, init_char='\\n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11865723",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
