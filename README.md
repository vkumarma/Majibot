# Majibot Implementation

The implementation involves creation of vocab object for questions or source and for answers or target. These vocab objects provide easy way to convert word to integers or indexes and back to actual words or strings. This is important because model only processes numbers or words in integer form. Where these integers also in way represent one hot encoding vectors. The integers/words are passed through the embedding layer and every integer or word corresponds to embedding layer or embedding weight matrix(under the hood) row, and we get a vector representation for that integer/word. These vector representations then pass through the LSTM layer in the encoder, the LSTM layer have hidden and cell states which are initially tensors of zeros,and after every sequence processing the new hidden and cell state become a input as well for the next sequence or word. We donâ€™t care about the generated output of the encoder.However, the last hidden and cell state after processing a input sentence or query completely is sent to the decoder as initial its hidden and cell state, and serves the purpose of context vector(query representation). The decoder also consists of embedding layer, LSTM layer, linear layer and softmax. The first input to the decoder is the <sos> token, which passes through the embedding layer to the LSTM layer to the linear layer and we get <sos> token mapped to predictions against the target vocab size, and select the top1 integer(most likely to be the next word) using output.argmax(1). Then in training we use teacher_forcing with ratio of 0.5 which acts as probability so for the next iteration in the decoder we sometimes use the predicted token as input and sometimes use the actual ground truth or target word/integer. In this way we generate the whole output from the decoder, and when calculating the loss we get rid of both 0th index element in both generated output tensor and target tensor, and calculate the loss. The model is trained on batches of data, where each batch refers to a sentence or question and same process applies to the answer or target as well. The creation of batches process was heavily tested to make sure that getting right amount of batches and that both src and trg have <sos> and <eos> tokens and even padding or <pad> token so that the batch size is consistent through all src batches and trg batches. The batches length or first dimension(row) for both src and trg vary as depends on the max length sentence. In a nutshell, we basically get a probability distribution for the most likely next words in response to a question(certain words). Those words are then joined together to form a complete answer or response to a question.
