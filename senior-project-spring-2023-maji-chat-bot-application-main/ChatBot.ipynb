{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1471,
   "id": "2b3d94e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default tokens\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word_2_index = {\"PAD\":PAD_token, \"SOS\":SOS_token, \"EOS\":EOS_token} # words converted to index\n",
    "        self.word_2_count = {} # unique word count\n",
    "        self.index_2_word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count SOS, EOS, PAD\n",
    "    \n",
    "    def build_vocab(self, data_set):\n",
    "        for s in data_set:\n",
    "            self.add_sentence(s)\n",
    "            \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence:\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word_2_index:\n",
    "            self.word_2_index[word] = self.num_words\n",
    "            self.word_2_count[word] = 1\n",
    "            self.index_2_word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word_2_count[word] += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1472,
   "id": "d5cebc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import re\n",
    "from torchtext import datasets\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# from nltk.stem import PorterStemme\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "def loadDF(data_iter): # loading the dataset into Pandas Data frame for processing.\n",
    "    data = {\"Question\": list(), \"Answer\": list()}\n",
    "    for _, question, answer, _ in data_iter:\n",
    "        if len(question) != 0 and len(answer[0]) != 0:\n",
    "            data[\"Question\"].append(question)\n",
    "            data[\"Answer\"].append(answer[0])\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def prepare_text(sentence): # the text is cleaned using tokenizer\n",
    "    \n",
    "    # using a stemmer to reduce the number of words that serve as an input\n",
    "    # Lowercase, and remove non-letter characters\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r\"([.!?])\", r\" \\1\", sentence)\n",
    "    sentence = re.sub(r\"\\s+\", r\" \", sentence).strip()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    new_tokens = [token for token in tokens if not token.isdigit()]\n",
    "    return new_tokens\n",
    "\n",
    "def filter(temp):\n",
    "    filtered_q = list()\n",
    "    filtered_a = list()\n",
    "    \n",
    "    q = temp[\"Question\"].to_list()\n",
    "    a = temp[\"Answer\"].to_list()\n",
    "    \n",
    "    for i in range(0, len(q)):\n",
    "        temp_q = len(q[i])\n",
    "        temp_a = len(a[i])\n",
    "        if temp_q < MAX_LENGTH and temp_a < MAX_LENGTH:\n",
    "            filtered_q.append(q[i])\n",
    "            filtered_a.append(a[i])\n",
    "    \n",
    "    return filtered_q, filtered_a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1516,
   "id": "67031013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9097\n",
      "\n",
      "\n",
      "9097\n"
     ]
    }
   ],
   "source": [
    "# loading data using torch text\n",
    "train_iter, test_iter = datasets.SQuAD2(\"./data/SQuAD2\", split=(\"train\", \"dev\"))\n",
    "train_df = loadDF(train_iter).iloc[:20000]\n",
    "test_df = loadDF(test_iter).iloc[:500]\n",
    "\n",
    "# prepare_text function is applied to every sentence\n",
    "train_df[\"Question\"] = train_df[\"Question\"].apply(prepare_text) \n",
    "train_df[\"Answer\"] = train_df[\"Answer\"].apply(prepare_text)\n",
    "\n",
    "test_df[\"Question\"] = test_df[\"Question\"].apply(prepare_text)\n",
    "test_df[\"Answer\"] = test_df[\"Answer\"].apply(prepare_text)\n",
    "\n",
    "a , b = filter(train_df)\n",
    "print(len(a))\n",
    "print(\"\\n\")\n",
    "print(len(b))\n",
    "\n",
    "\n",
    "src_vocab = Vocab(\"Question_Vocab\")\n",
    "src_vocab.build_vocab(a)\n",
    "\n",
    "trg_vocab = Vocab(\"Answer_Vocab\")\n",
    "trg_vocab.build_vocab(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1474,
   "id": "b69504b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8722\n",
      "7181\n"
     ]
    }
   ],
   "source": [
    "print(src_vocab.num_words)\n",
    "print(trg_vocab.num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1475,
   "id": "df3495f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df[\"Question\"]) // 128) \n",
    "print(1000 // 128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1477,
   "id": "96b86aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexify_padding(temp, vocab): # temp - lists, vocab - object different for q(src) and a(trg)\n",
    "#     print(vocab.name)\n",
    "    new_lists = list()\n",
    "    max_len = 0\n",
    "    for s in temp:\n",
    "        temp_l = len(s)\n",
    "        if temp_l > max_len:\n",
    "            max_len = temp_l\n",
    "            \n",
    "    max_len += 2 # to add <sos> and <eos> tokens\n",
    "    for l in temp:\n",
    "        new_l = [vocab.word_2_index[word] for word in l]\n",
    "        new_l.insert(0, SOS_token) # adding <sos> token\n",
    "        new_l.append(EOS_token) # adding <eos> token\n",
    "        \n",
    "        while len(new_l) < max_len: # adding <pad> tokens to balance\n",
    "            new_l.append(PAD_token)\n",
    "            \n",
    "        new_lists.append(new_l)\n",
    "    \n",
    "    return new_lists\n",
    "\n",
    "\n",
    "#batch_size = 128\n",
    "def get_batches(src, trg, batch_size=128,src_vocab=src_vocab,trg_vocab=trg_vocab):\n",
    "    n_batches = len(src) // batch_size # integer division\n",
    "    for i in range(0, n_batches):\n",
    "        src_batch = src[i*batch_size:(batch_size*(i+1))]\n",
    "        src_batch = np.array(indexify_padding(src_batch, src_vocab)).T\n",
    "        \n",
    "        trg_batch = trg[i*batch_size:(batch_size*(i+1))]\n",
    "        trg_batch = np.array(indexify_padding(trg_batch, trg_vocab)).T\n",
    "        \n",
    "        yield src_batch, trg_batch, n_batches\n",
    "        \n",
    "        \n",
    "# SRC = train_df[\"Question\"].to_list()\n",
    "# TRG = train_df[\"Answer\"].to_list()\n",
    "\n",
    "SRC, TRG = filter(train_df)\n",
    "test_SRC, test_TRG = filter(test_df)\n",
    "\n",
    "test_src_vocab = Vocab(\"Test src\")\n",
    "test_src_vocab.build_vocab(test_SRC)\n",
    "\n",
    "test_trg_vocab = Vocab(\"Test trg\")\n",
    "test_trg_vocab.build_vocab(test_TRG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1478,
   "id": "0e029de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, n_layers, dropout):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.embedding_dim = embedding_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(self.input_size, self.embedding_dim)\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_size, n_layers, dropout=dropout) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, i):\n",
    "        \n",
    "        embedding = self.dropout(self.embedding(i))\n",
    "        o, (h,c) = self.lstm(embedding)      \n",
    "        \n",
    "        return h, c\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "      \n",
    "    def __init__(self, output_size, embedding_size,hidden_size, n_layers, dropout):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # The LSTM produces an output by passing the hidden state to the   Linear layer\n",
    "        self.embedding = nn.Embedding(self.output_size, self.embedding_size)\n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, n_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, i, h, c): # i - input, h - hidden state, c - cell state\n",
    "        \n",
    "        i = i.unsqueeze(0)\n",
    "        embedding = self.dropout(self.embedding(i))\n",
    "        o, (h, c) = self.lstm(embedding, (h, c))\n",
    "        o = self.out(o.squeeze(0)) # o - prediction\n",
    "        o = self.softmax(o)\n",
    "        \n",
    "        return o, h, c\n",
    "        \n",
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hidden_size == decoder.hidden_size, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "    \n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):  \n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_size\n",
    "        \n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        \n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            outputs[t] = output\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            input = trg[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1479,
   "id": "33c1344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "INPUT_DIM = src_vocab.num_words\n",
    "OUTPUT_DIM = trg_vocab.num_words\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 3\n",
    "ENC_DROPOUT = 0.2\n",
    "DEC_DROPOUT = 0.2\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1480,
   "id": "53b7bc44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(8722, 256)\n",
       "    (lstm): LSTM(256, 512, num_layers=3, dropout=0.2)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(7181, 256)\n",
       "    (lstm): LSTM(256, 512, num_layers=3, dropout=0.2)\n",
       "    (out): Linear(in_features=512, out_features=7181, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (softmax): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1481,
   "id": "f7d751ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 19,313,933 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1482,
   "id": "7e253e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "criterion =  nn.CrossEntropyLoss(ignore_index = PAD_token)\n",
    "# criterion = nn.NLLLoss(ignore_index=PAD_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1483,
   "id": "ec4abc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, trg, n_batches in get_batches(SRC, TRG):\n",
    "        \n",
    "        src = torch.from_numpy(src)\n",
    "        trg = torch.from_numpy(trg)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        \n",
    "        # flattening and getting rid of <sos> and 0 in trg and output respectively\n",
    "        output = output[1:].view(-1, output.shape[-1]) \n",
    "        trg = trg[1:].contiguous().view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1484,
   "id": "bfecfcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, trg, n_batches in get_batches(test_SRC, test_TRG, 128, test_src_vocab, test_trg_vocab):\n",
    "            src = torch.from_numpy(src)\n",
    "            trg = torch.from_numpy(trg)\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].contiguous().view(-1)\n",
    "            \n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1485,
   "id": "71e8035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1486,
   "id": "4698a039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 28s\n",
      "\tTrain Loss: 6.471 | Train PPL: 646.381\n",
      "\t Val. Loss: 6.315 |  Val. PPL: 552.570\n",
      "Epoch: 02 | Time: 0m 26s\n",
      "\tTrain Loss: 5.927 | Train PPL: 374.988\n",
      "\t Val. Loss: 6.214 |  Val. PPL: 499.730\n",
      "Epoch: 03 | Time: 0m 26s\n",
      "\tTrain Loss: 5.891 | Train PPL: 361.747\n",
      "\t Val. Loss: 6.314 |  Val. PPL: 552.391\n",
      "Epoch: 04 | Time: 0m 26s\n",
      "\tTrain Loss: 5.885 | Train PPL: 359.509\n",
      "\t Val. Loss: 6.422 |  Val. PPL: 615.207\n",
      "Epoch: 05 | Time: 0m 26s\n",
      "\tTrain Loss: 5.792 | Train PPL: 327.574\n",
      "\t Val. Loss: 6.348 |  Val. PPL: 571.355\n",
      "Epoch: 06 | Time: 0m 26s\n",
      "\tTrain Loss: 5.717 | Train PPL: 303.923\n",
      "\t Val. Loss: 6.385 |  Val. PPL: 592.819\n",
      "Epoch: 07 | Time: 0m 26s\n",
      "\tTrain Loss: 5.683 | Train PPL: 293.784\n",
      "\t Val. Loss: 6.419 |  Val. PPL: 613.354\n",
      "Epoch: 08 | Time: 0m 26s\n",
      "\tTrain Loss: 5.658 | Train PPL: 286.564\n",
      "\t Val. Loss: 6.479 |  Val. PPL: 651.420\n",
      "Epoch: 09 | Time: 0m 26s\n",
      "\tTrain Loss: 5.640 | Train PPL: 281.452\n",
      "\t Val. Loss: 6.497 |  Val. PPL: 662.988\n",
      "Epoch: 10 | Time: 0m 26s\n",
      "\tTrain Loss: 5.593 | Train PPL: 268.570\n",
      "\t Val. Loss: 6.512 |  Val. PPL: 673.504\n",
      "Epoch: 11 | Time: 0m 26s\n",
      "\tTrain Loss: 5.475 | Train PPL: 238.687\n",
      "\t Val. Loss: 6.506 |  Val. PPL: 669.034\n",
      "Epoch: 12 | Time: 0m 27s\n",
      "\tTrain Loss: 5.330 | Train PPL: 206.489\n",
      "\t Val. Loss: 6.488 |  Val. PPL: 657.073\n",
      "Epoch: 13 | Time: 0m 26s\n",
      "\tTrain Loss: 5.075 | Train PPL: 159.986\n",
      "\t Val. Loss: 6.369 |  Val. PPL: 583.592\n",
      "Epoch: 14 | Time: 0m 26s\n",
      "\tTrain Loss: 4.939 | Train PPL: 139.628\n",
      "\t Val. Loss: 6.425 |  Val. PPL: 617.136\n",
      "Epoch: 15 | Time: 0m 26s\n",
      "\tTrain Loss: 4.822 | Train PPL: 124.259\n",
      "\t Val. Loss: 6.591 |  Val. PPL: 728.670\n",
      "Epoch: 16 | Time: 0m 26s\n",
      "\tTrain Loss: 4.757 | Train PPL: 116.390\n",
      "\t Val. Loss: 6.557 |  Val. PPL: 704.229\n",
      "Epoch: 17 | Time: 0m 26s\n",
      "\tTrain Loss: 4.681 | Train PPL: 107.932\n",
      "\t Val. Loss: 6.602 |  Val. PPL: 736.793\n",
      "Epoch: 18 | Time: 0m 27s\n",
      "\tTrain Loss: 4.684 | Train PPL: 108.156\n",
      "\t Val. Loss: 6.641 |  Val. PPL: 765.630\n",
      "Epoch: 19 | Time: 0m 26s\n",
      "\tTrain Loss: 4.595 | Train PPL:  99.005\n",
      "\t Val. Loss: 6.569 |  Val. PPL: 712.442\n",
      "Epoch: 20 | Time: 0m 26s\n",
      "\tTrain Loss: 4.549 | Train PPL:  94.503\n",
      "\t Val. Loss: 6.752 |  Val. PPL: 855.710\n",
      "Epoch: 21 | Time: 0m 26s\n",
      "\tTrain Loss: 4.517 | Train PPL:  91.571\n",
      "\t Val. Loss: 6.994 |  Val. PPL: 1090.412\n",
      "Epoch: 22 | Time: 0m 26s\n",
      "\tTrain Loss: 4.699 | Train PPL: 109.859\n",
      "\t Val. Loss: 7.133 |  Val. PPL: 1252.492\n",
      "Epoch: 23 | Time: 0m 26s\n",
      "\tTrain Loss: 4.555 | Train PPL:  95.094\n",
      "\t Val. Loss: 6.897 |  Val. PPL: 988.819\n",
      "Epoch: 24 | Time: 0m 26s\n",
      "\tTrain Loss: 4.567 | Train PPL:  96.289\n",
      "\t Val. Loss: 6.995 |  Val. PPL: 1091.586\n",
      "Epoch: 25 | Time: 0m 26s\n",
      "\tTrain Loss: 4.413 | Train PPL:  82.516\n",
      "\t Val. Loss: 7.184 |  Val. PPL: 1318.519\n",
      "Epoch: 26 | Time: 0m 26s\n",
      "\tTrain Loss: 4.332 | Train PPL:  76.111\n",
      "\t Val. Loss: 7.100 |  Val. PPL: 1211.521\n",
      "Epoch: 27 | Time: 0m 26s\n",
      "\tTrain Loss: 4.452 | Train PPL:  85.803\n",
      "\t Val. Loss: 7.424 |  Val. PPL: 1675.101\n",
      "Epoch: 28 | Time: 0m 26s\n",
      "\tTrain Loss: 4.305 | Train PPL:  74.065\n",
      "\t Val. Loss: 7.244 |  Val. PPL: 1400.020\n",
      "Epoch: 29 | Time: 0m 26s\n",
      "\tTrain Loss: 4.711 | Train PPL: 111.139\n",
      "\t Val. Loss: 7.312 |  Val. PPL: 1498.582\n",
      "Epoch: 30 | Time: 0m 27s\n",
      "\tTrain Loss: 4.417 | Train PPL:  82.840\n",
      "\t Val. Loss: 7.089 |  Val. PPL: 1198.494\n",
      "Epoch: 31 | Time: 0m 26s\n",
      "\tTrain Loss: 4.444 | Train PPL:  85.131\n",
      "\t Val. Loss: 7.507 |  Val. PPL: 1820.288\n",
      "Epoch: 32 | Time: 0m 26s\n",
      "\tTrain Loss: 4.495 | Train PPL:  89.581\n",
      "\t Val. Loss: 7.289 |  Val. PPL: 1463.757\n",
      "Epoch: 33 | Time: 0m 26s\n",
      "\tTrain Loss: 4.447 | Train PPL:  85.391\n",
      "\t Val. Loss: 7.390 |  Val. PPL: 1619.212\n",
      "Epoch: 34 | Time: 0m 26s\n",
      "\tTrain Loss: 4.195 | Train PPL:  66.339\n",
      "\t Val. Loss: 7.590 |  Val. PPL: 1979.246\n",
      "Epoch: 35 | Time: 0m 26s\n",
      "\tTrain Loss: 4.205 | Train PPL:  67.019\n",
      "\t Val. Loss: 7.537 |  Val. PPL: 1877.012\n",
      "Epoch: 36 | Time: 0m 26s\n",
      "\tTrain Loss: 4.062 | Train PPL:  58.091\n",
      "\t Val. Loss: 7.567 |  Val. PPL: 1932.513\n",
      "Epoch: 37 | Time: 0m 26s\n",
      "\tTrain Loss: 3.990 | Train PPL:  54.038\n",
      "\t Val. Loss: 7.519 |  Val. PPL: 1843.585\n",
      "Epoch: 38 | Time: 0m 27s\n",
      "\tTrain Loss: 3.907 | Train PPL:  49.731\n",
      "\t Val. Loss: 7.768 |  Val. PPL: 2364.057\n",
      "Epoch: 39 | Time: 0m 27s\n",
      "\tTrain Loss: 3.948 | Train PPL:  51.815\n",
      "\t Val. Loss: 7.607 |  Val. PPL: 2013.223\n",
      "Epoch: 40 | Time: 0m 26s\n",
      "\tTrain Loss: 3.879 | Train PPL:  48.366\n",
      "\t Val. Loss: 7.932 |  Val. PPL: 2785.585\n",
      "Epoch: 41 | Time: 0m 26s\n",
      "\tTrain Loss: 3.819 | Train PPL:  45.555\n",
      "\t Val. Loss: 7.782 |  Val. PPL: 2397.978\n",
      "Epoch: 42 | Time: 0m 26s\n",
      "\tTrain Loss: 3.965 | Train PPL:  52.712\n",
      "\t Val. Loss: 8.009 |  Val. PPL: 3008.966\n",
      "Epoch: 43 | Time: 0m 26s\n",
      "\tTrain Loss: 3.791 | Train PPL:  44.312\n",
      "\t Val. Loss: 7.938 |  Val. PPL: 2802.794\n",
      "Epoch: 44 | Time: 0m 26s\n",
      "\tTrain Loss: 3.749 | Train PPL:  42.479\n",
      "\t Val. Loss: 8.049 |  Val. PPL: 3130.617\n",
      "Epoch: 45 | Time: 0m 26s\n",
      "\tTrain Loss: 3.603 | Train PPL:  36.710\n",
      "\t Val. Loss: 8.273 |  Val. PPL: 3917.996\n",
      "Epoch: 46 | Time: 0m 26s\n",
      "\tTrain Loss: 3.625 | Train PPL:  37.536\n",
      "\t Val. Loss: 8.268 |  Val. PPL: 3896.432\n",
      "Epoch: 47 | Time: 0m 25s\n",
      "\tTrain Loss: 3.638 | Train PPL:  38.003\n",
      "\t Val. Loss: 8.558 |  Val. PPL: 5205.864\n",
      "Epoch: 48 | Time: 0m 25s\n",
      "\tTrain Loss: 3.619 | Train PPL:  37.313\n",
      "\t Val. Loss: 8.321 |  Val. PPL: 4108.092\n",
      "Epoch: 49 | Time: 0m 25s\n",
      "\tTrain Loss: 3.510 | Train PPL:  33.451\n",
      "\t Val. Loss: 8.344 |  Val. PPL: 4202.879\n",
      "Epoch: 50 | Time: 0m 25s\n",
      "\tTrain Loss: 3.591 | Train PPL:  36.254\n",
      "\t Val. Loss: 8.194 |  Val. PPL: 3619.696\n",
      "Epoch: 51 | Time: 0m 29s\n",
      "\tTrain Loss: 3.521 | Train PPL:  33.835\n",
      "\t Val. Loss: 8.412 |  Val. PPL: 4501.881\n",
      "Epoch: 52 | Time: 0m 27s\n",
      "\tTrain Loss: 3.432 | Train PPL:  30.948\n",
      "\t Val. Loss: 8.416 |  Val. PPL: 4519.966\n",
      "Epoch: 53 | Time: 0m 27s\n",
      "\tTrain Loss: 3.394 | Train PPL:  29.797\n",
      "\t Val. Loss: 8.922 |  Val. PPL: 7493.084\n",
      "Epoch: 54 | Time: 0m 28s\n",
      "\tTrain Loss: 3.377 | Train PPL:  29.270\n",
      "\t Val. Loss: 8.730 |  Val. PPL: 6183.024\n",
      "Epoch: 55 | Time: 0m 27s\n",
      "\tTrain Loss: 3.421 | Train PPL:  30.592\n",
      "\t Val. Loss: 8.634 |  Val. PPL: 5617.142\n",
      "Epoch: 56 | Time: 0m 27s\n",
      "\tTrain Loss: 3.281 | Train PPL:  26.616\n",
      "\t Val. Loss: 8.625 |  Val. PPL: 5566.609\n",
      "Epoch: 57 | Time: 0m 27s\n",
      "\tTrain Loss: 3.282 | Train PPL:  26.632\n",
      "\t Val. Loss: 8.812 |  Val. PPL: 6712.121\n",
      "Epoch: 58 | Time: 0m 26s\n",
      "\tTrain Loss: 3.240 | Train PPL:  25.544\n",
      "\t Val. Loss: 8.846 |  Val. PPL: 6944.419\n",
      "Epoch: 59 | Time: 0m 27s\n",
      "\tTrain Loss: 3.182 | Train PPL:  24.093\n",
      "\t Val. Loss: 8.913 |  Val. PPL: 7429.705\n",
      "Epoch: 60 | Time: 0m 27s\n",
      "\tTrain Loss: 3.162 | Train PPL:  23.610\n",
      "\t Val. Loss: 8.842 |  Val. PPL: 6919.404\n",
      "Epoch: 61 | Time: 0m 27s\n",
      "\tTrain Loss: 3.154 | Train PPL:  23.438\n",
      "\t Val. Loss: 8.825 |  Val. PPL: 6800.496\n",
      "Epoch: 62 | Time: 0m 26s\n",
      "\tTrain Loss: 3.104 | Train PPL:  22.287\n",
      "\t Val. Loss: 9.006 |  Val. PPL: 8147.788\n",
      "Epoch: 63 | Time: 0m 27s\n",
      "\tTrain Loss: 3.146 | Train PPL:  23.243\n",
      "\t Val. Loss: 9.131 |  Val. PPL: 9233.912\n",
      "Epoch: 64 | Time: 0m 26s\n",
      "\tTrain Loss: 3.115 | Train PPL:  22.534\n",
      "\t Val. Loss: 9.267 |  Val. PPL: 10587.298\n",
      "Epoch: 65 | Time: 0m 26s\n",
      "\tTrain Loss: 3.098 | Train PPL:  22.159\n",
      "\t Val. Loss: 9.471 |  Val. PPL: 12982.088\n",
      "Epoch: 66 | Time: 0m 26s\n",
      "\tTrain Loss: 3.027 | Train PPL:  20.630\n",
      "\t Val. Loss: 9.316 |  Val. PPL: 11119.662\n",
      "Epoch: 67 | Time: 0m 27s\n",
      "\tTrain Loss: 2.958 | Train PPL:  19.252\n",
      "\t Val. Loss: 9.530 |  Val. PPL: 13761.179\n",
      "Epoch: 68 | Time: 0m 28s\n",
      "\tTrain Loss: 2.986 | Train PPL:  19.802\n",
      "\t Val. Loss: 9.546 |  Val. PPL: 13988.421\n",
      "Epoch: 69 | Time: 0m 27s\n",
      "\tTrain Loss: 2.897 | Train PPL:  18.111\n",
      "\t Val. Loss: 9.635 |  Val. PPL: 15292.306\n",
      "Epoch: 70 | Time: 0m 28s\n",
      "\tTrain Loss: 2.969 | Train PPL:  19.473\n",
      "\t Val. Loss: 9.396 |  Val. PPL: 12034.521\n",
      "Epoch: 71 | Time: 0m 27s\n",
      "\tTrain Loss: 2.910 | Train PPL:  18.364\n",
      "\t Val. Loss: 9.555 |  Val. PPL: 14118.424\n",
      "Epoch: 72 | Time: 0m 26s\n",
      "\tTrain Loss: 2.874 | Train PPL:  17.707\n",
      "\t Val. Loss: 9.758 |  Val. PPL: 17286.925\n",
      "Epoch: 73 | Time: 0m 27s\n",
      "\tTrain Loss: 2.868 | Train PPL:  17.607\n",
      "\t Val. Loss: 9.709 |  Val. PPL: 16467.258\n",
      "Epoch: 74 | Time: 0m 27s\n",
      "\tTrain Loss: 2.822 | Train PPL:  16.810\n",
      "\t Val. Loss: 9.620 |  Val. PPL: 15058.524\n",
      "Epoch: 75 | Time: 0m 26s\n",
      "\tTrain Loss: 2.778 | Train PPL:  16.091\n",
      "\t Val. Loss: 9.751 |  Val. PPL: 17165.733\n",
      "Epoch: 76 | Time: 0m 26s\n",
      "\tTrain Loss: 2.746 | Train PPL:  15.586\n",
      "\t Val. Loss: 9.861 |  Val. PPL: 19176.771\n",
      "Epoch: 77 | Time: 0m 26s\n",
      "\tTrain Loss: 2.665 | Train PPL:  14.361\n",
      "\t Val. Loss: 10.048 |  Val. PPL: 23110.146\n",
      "Epoch: 78 | Time: 0m 26s\n",
      "\tTrain Loss: 2.715 | Train PPL:  15.105\n",
      "\t Val. Loss: 9.791 |  Val. PPL: 17869.671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79 | Time: 0m 26s\n",
      "\tTrain Loss: 2.653 | Train PPL:  14.195\n",
      "\t Val. Loss: 10.067 |  Val. PPL: 23551.080\n",
      "Epoch: 80 | Time: 0m 26s\n",
      "\tTrain Loss: 2.585 | Train PPL:  13.263\n",
      "\t Val. Loss: 10.147 |  Val. PPL: 25510.510\n",
      "Epoch: 81 | Time: 0m 28s\n",
      "\tTrain Loss: 2.639 | Train PPL:  13.994\n",
      "\t Val. Loss: 10.105 |  Val. PPL: 24473.652\n",
      "Epoch: 82 | Time: 0m 27s\n",
      "\tTrain Loss: 2.507 | Train PPL:  12.263\n",
      "\t Val. Loss: 10.171 |  Val. PPL: 26136.828\n",
      "Epoch: 83 | Time: 0m 27s\n",
      "\tTrain Loss: 2.686 | Train PPL:  14.669\n",
      "\t Val. Loss: 10.247 |  Val. PPL: 28189.210\n",
      "Epoch: 84 | Time: 0m 26s\n",
      "\tTrain Loss: 2.571 | Train PPL:  13.074\n",
      "\t Val. Loss: 10.202 |  Val. PPL: 26954.749\n",
      "Epoch: 85 | Time: 0m 27s\n",
      "\tTrain Loss: 2.482 | Train PPL:  11.960\n",
      "\t Val. Loss: 10.364 |  Val. PPL: 31684.733\n",
      "Epoch: 86 | Time: 0m 26s\n",
      "\tTrain Loss: 2.457 | Train PPL:  11.668\n",
      "\t Val. Loss: 10.314 |  Val. PPL: 30150.024\n",
      "Epoch: 87 | Time: 0m 26s\n",
      "\tTrain Loss: 2.446 | Train PPL:  11.543\n",
      "\t Val. Loss: 10.390 |  Val. PPL: 32544.004\n",
      "Epoch: 88 | Time: 0m 26s\n",
      "\tTrain Loss: 2.354 | Train PPL:  10.531\n",
      "\t Val. Loss: 10.402 |  Val. PPL: 32936.605\n",
      "Epoch: 89 | Time: 0m 26s\n",
      "\tTrain Loss: 2.371 | Train PPL:  10.710\n",
      "\t Val. Loss: 10.490 |  Val. PPL: 35964.094\n",
      "Epoch: 90 | Time: 0m 26s\n",
      "\tTrain Loss: 2.328 | Train PPL:  10.262\n",
      "\t Val. Loss: 10.442 |  Val. PPL: 34269.495\n",
      "Epoch: 91 | Time: 0m 26s\n",
      "\tTrain Loss: 2.390 | Train PPL:  10.914\n",
      "\t Val. Loss: 10.632 |  Val. PPL: 41442.055\n",
      "Epoch: 92 | Time: 0m 26s\n",
      "\tTrain Loss: 2.281 | Train PPL:   9.788\n",
      "\t Val. Loss: 10.508 |  Val. PPL: 36610.732\n",
      "Epoch: 93 | Time: 0m 26s\n",
      "\tTrain Loss: 2.164 | Train PPL:   8.703\n",
      "\t Val. Loss: 10.526 |  Val. PPL: 37254.355\n",
      "Epoch: 94 | Time: 0m 26s\n",
      "\tTrain Loss: 2.154 | Train PPL:   8.622\n",
      "\t Val. Loss: 10.642 |  Val. PPL: 41842.360\n",
      "Epoch: 95 | Time: 0m 26s\n",
      "\tTrain Loss: 2.106 | Train PPL:   8.215\n",
      "\t Val. Loss: 10.810 |  Val. PPL: 49535.073\n",
      "Epoch: 96 | Time: 0m 26s\n",
      "\tTrain Loss: 2.112 | Train PPL:   8.261\n",
      "\t Val. Loss: 10.943 |  Val. PPL: 56546.773\n",
      "Epoch: 97 | Time: 0m 26s\n",
      "\tTrain Loss: 2.162 | Train PPL:   8.691\n",
      "\t Val. Loss: 10.686 |  Val. PPL: 43730.394\n",
      "Epoch: 98 | Time: 0m 27s\n",
      "\tTrain Loss: 2.065 | Train PPL:   7.885\n",
      "\t Val. Loss: 11.112 |  Val. PPL: 66937.847\n",
      "Epoch: 99 | Time: 0m 26s\n",
      "\tTrain Loss: 2.034 | Train PPL:   7.647\n",
      "\t Val. Loss: 10.881 |  Val. PPL: 53140.791\n",
      "Epoch: 100 | Time: 0m 26s\n",
      "\tTrain Loss: 2.060 | Train PPL:   7.847\n",
      "\t Val. Loss: 11.202 |  Val. PPL: 73257.749\n",
      "Epoch: 101 | Time: 0m 26s\n",
      "\tTrain Loss: 1.982 | Train PPL:   7.260\n",
      "\t Val. Loss: 10.950 |  Val. PPL: 56975.819\n",
      "Epoch: 102 | Time: 0m 26s\n",
      "\tTrain Loss: 2.124 | Train PPL:   8.363\n",
      "\t Val. Loss: 11.184 |  Val. PPL: 71986.965\n",
      "Epoch: 103 | Time: 0m 26s\n",
      "\tTrain Loss: 1.925 | Train PPL:   6.854\n",
      "\t Val. Loss: 11.033 |  Val. PPL: 61899.891\n",
      "Epoch: 104 | Time: 0m 26s\n",
      "\tTrain Loss: 1.977 | Train PPL:   7.222\n",
      "\t Val. Loss: 11.105 |  Val. PPL: 66525.967\n",
      "Epoch: 105 | Time: 0m 26s\n",
      "\tTrain Loss: 1.908 | Train PPL:   6.738\n",
      "\t Val. Loss: 11.046 |  Val. PPL: 62716.572\n",
      "Epoch: 106 | Time: 0m 26s\n",
      "\tTrain Loss: 1.873 | Train PPL:   6.506\n",
      "\t Val. Loss: 11.145 |  Val. PPL: 69203.642\n",
      "Epoch: 107 | Time: 0m 26s\n",
      "\tTrain Loss: 1.841 | Train PPL:   6.305\n",
      "\t Val. Loss: 11.392 |  Val. PPL: 88594.555\n",
      "Epoch: 108 | Time: 0m 26s\n",
      "\tTrain Loss: 1.940 | Train PPL:   6.961\n",
      "\t Val. Loss: 11.323 |  Val. PPL: 82695.291\n",
      "Epoch: 109 | Time: 0m 26s\n",
      "\tTrain Loss: 2.021 | Train PPL:   7.545\n",
      "\t Val. Loss: 11.185 |  Val. PPL: 72035.312\n",
      "Epoch: 110 | Time: 0m 26s\n",
      "\tTrain Loss: 1.866 | Train PPL:   6.465\n",
      "\t Val. Loss: 11.667 |  Val. PPL: 116627.950\n",
      "Epoch: 111 | Time: 0m 27s\n",
      "\tTrain Loss: 1.746 | Train PPL:   5.733\n",
      "\t Val. Loss: 11.530 |  Val. PPL: 101720.535\n",
      "Epoch: 112 | Time: 0m 26s\n",
      "\tTrain Loss: 1.706 | Train PPL:   5.505\n",
      "\t Val. Loss: 11.729 |  Val. PPL: 124173.488\n",
      "Epoch: 113 | Time: 0m 26s\n",
      "\tTrain Loss: 1.720 | Train PPL:   5.585\n",
      "\t Val. Loss: 11.587 |  Val. PPL: 107738.722\n",
      "Epoch: 114 | Time: 0m 26s\n",
      "\tTrain Loss: 1.663 | Train PPL:   5.273\n",
      "\t Val. Loss: 11.675 |  Val. PPL: 117641.922\n",
      "Epoch: 115 | Time: 0m 26s\n",
      "\tTrain Loss: 1.571 | Train PPL:   4.809\n",
      "\t Val. Loss: 11.653 |  Val. PPL: 115032.978\n",
      "Epoch: 116 | Time: 0m 26s\n",
      "\tTrain Loss: 1.588 | Train PPL:   4.895\n",
      "\t Val. Loss: 11.913 |  Val. PPL: 149125.895\n",
      "Epoch: 117 | Time: 0m 26s\n",
      "\tTrain Loss: 1.630 | Train PPL:   5.102\n",
      "\t Val. Loss: 11.593 |  Val. PPL: 108353.461\n",
      "Epoch: 118 | Time: 0m 26s\n",
      "\tTrain Loss: 1.598 | Train PPL:   4.941\n",
      "\t Val. Loss: 11.977 |  Val. PPL: 158994.891\n",
      "Epoch: 119 | Time: 0m 26s\n",
      "\tTrain Loss: 1.602 | Train PPL:   4.961\n",
      "\t Val. Loss: 12.044 |  Val. PPL: 170133.419\n",
      "Epoch: 120 | Time: 0m 26s\n",
      "\tTrain Loss: 1.462 | Train PPL:   4.316\n",
      "\t Val. Loss: 12.225 |  Val. PPL: 203834.235\n",
      "Epoch: 121 | Time: 0m 26s\n",
      "\tTrain Loss: 1.466 | Train PPL:   4.330\n",
      "\t Val. Loss: 12.134 |  Val. PPL: 186122.445\n",
      "Epoch: 122 | Time: 0m 26s\n",
      "\tTrain Loss: 1.421 | Train PPL:   4.139\n",
      "\t Val. Loss: 12.201 |  Val. PPL: 198917.500\n",
      "Epoch: 123 | Time: 0m 26s\n",
      "\tTrain Loss: 1.422 | Train PPL:   4.147\n",
      "\t Val. Loss: 12.115 |  Val. PPL: 182522.685\n",
      "Epoch: 124 | Time: 0m 26s\n",
      "\tTrain Loss: 1.377 | Train PPL:   3.963\n",
      "\t Val. Loss: 12.377 |  Val. PPL: 237362.582\n",
      "Epoch: 125 | Time: 0m 26s\n",
      "\tTrain Loss: 1.328 | Train PPL:   3.773\n",
      "\t Val. Loss: 12.185 |  Val. PPL: 195856.357\n",
      "Epoch: 126 | Time: 0m 27s\n",
      "\tTrain Loss: 1.327 | Train PPL:   3.771\n",
      "\t Val. Loss: 12.309 |  Val. PPL: 221670.553\n",
      "Epoch: 127 | Time: 0m 26s\n",
      "\tTrain Loss: 1.276 | Train PPL:   3.583\n",
      "\t Val. Loss: 12.344 |  Val. PPL: 229559.791\n",
      "Epoch: 128 | Time: 0m 26s\n",
      "\tTrain Loss: 1.235 | Train PPL:   3.439\n",
      "\t Val. Loss: 12.298 |  Val. PPL: 219339.719\n",
      "Epoch: 129 | Time: 0m 26s\n",
      "\tTrain Loss: 1.185 | Train PPL:   3.271\n",
      "\t Val. Loss: 12.436 |  Val. PPL: 251762.662\n",
      "Epoch: 130 | Time: 0m 26s\n",
      "\tTrain Loss: 1.159 | Train PPL:   3.188\n",
      "\t Val. Loss: 12.588 |  Val. PPL: 293027.365\n",
      "Epoch: 131 | Time: 0m 26s\n",
      "\tTrain Loss: 1.150 | Train PPL:   3.157\n",
      "\t Val. Loss: 12.667 |  Val. PPL: 317091.135\n",
      "Epoch: 132 | Time: 0m 26s\n",
      "\tTrain Loss: 1.173 | Train PPL:   3.230\n",
      "\t Val. Loss: 12.878 |  Val. PPL: 391641.556\n",
      "Epoch: 133 | Time: 0m 26s\n",
      "\tTrain Loss: 1.169 | Train PPL:   3.219\n",
      "\t Val. Loss: 12.883 |  Val. PPL: 393676.415\n",
      "Epoch: 134 | Time: 0m 26s\n",
      "\tTrain Loss: 1.177 | Train PPL:   3.246\n",
      "\t Val. Loss: 12.767 |  Val. PPL: 350470.449\n",
      "Epoch: 135 | Time: 0m 26s\n",
      "\tTrain Loss: 1.087 | Train PPL:   2.966\n",
      "\t Val. Loss: 12.942 |  Val. PPL: 417593.439\n",
      "Epoch: 136 | Time: 0m 26s\n",
      "\tTrain Loss: 1.119 | Train PPL:   3.063\n",
      "\t Val. Loss: 12.867 |  Val. PPL: 387145.246\n",
      "Epoch: 137 | Time: 0m 26s\n",
      "\tTrain Loss: 1.087 | Train PPL:   2.964\n",
      "\t Val. Loss: 12.908 |  Val. PPL: 403383.628\n",
      "Epoch: 138 | Time: 0m 26s\n",
      "\tTrain Loss: 1.091 | Train PPL:   2.976\n",
      "\t Val. Loss: 13.184 |  Val. PPL: 531752.913\n",
      "Epoch: 139 | Time: 0m 26s\n",
      "\tTrain Loss: 1.039 | Train PPL:   2.828\n",
      "\t Val. Loss: 12.984 |  Val. PPL: 435302.347\n",
      "Epoch: 140 | Time: 0m 26s\n",
      "\tTrain Loss: 1.056 | Train PPL:   2.874\n",
      "\t Val. Loss: 13.314 |  Val. PPL: 605319.015\n",
      "Epoch: 141 | Time: 0m 26s\n",
      "\tTrain Loss: 1.008 | Train PPL:   2.739\n",
      "\t Val. Loss: 13.356 |  Val. PPL: 631650.491\n",
      "Epoch: 142 | Time: 0m 26s\n",
      "\tTrain Loss: 1.056 | Train PPL:   2.875\n",
      "\t Val. Loss: 13.330 |  Val. PPL: 615439.224\n",
      "Epoch: 143 | Time: 0m 26s\n",
      "\tTrain Loss: 1.114 | Train PPL:   3.046\n",
      "\t Val. Loss: 13.320 |  Val. PPL: 609347.322\n",
      "Epoch: 144 | Time: 0m 26s\n",
      "\tTrain Loss: 1.082 | Train PPL:   2.952\n",
      "\t Val. Loss: 13.361 |  Val. PPL: 634947.506\n",
      "Epoch: 145 | Time: 0m 26s\n",
      "\tTrain Loss: 0.984 | Train PPL:   2.675\n",
      "\t Val. Loss: 13.354 |  Val. PPL: 630404.175\n",
      "Epoch: 146 | Time: 0m 26s\n",
      "\tTrain Loss: 0.956 | Train PPL:   2.600\n",
      "\t Val. Loss: 13.479 |  Val. PPL: 714275.437\n",
      "Epoch: 147 | Time: 0m 26s\n",
      "\tTrain Loss: 0.897 | Train PPL:   2.453\n",
      "\t Val. Loss: 13.416 |  Val. PPL: 670832.039\n",
      "Epoch: 148 | Time: 0m 26s\n",
      "\tTrain Loss: 0.883 | Train PPL:   2.418\n",
      "\t Val. Loss: 13.496 |  Val. PPL: 726461.094\n",
      "Epoch: 149 | Time: 0m 26s\n",
      "\tTrain Loss: 0.812 | Train PPL:   2.253\n",
      "\t Val. Loss: 13.518 |  Val. PPL: 742478.941\n",
      "Epoch: 150 | Time: 0m 26s\n",
      "\tTrain Loss: 0.900 | Train PPL:   2.458\n",
      "\t Val. Loss: 13.636 |  Val. PPL: 835561.714\n",
      "Epoch: 151 | Time: 0m 26s\n",
      "\tTrain Loss: 0.771 | Train PPL:   2.162\n",
      "\t Val. Loss: 13.746 |  Val. PPL: 932485.875\n",
      "Epoch: 152 | Time: 0m 27s\n",
      "\tTrain Loss: 0.784 | Train PPL:   2.191\n",
      "\t Val. Loss: 13.857 |  Val. PPL: 1042615.048\n",
      "Epoch: 153 | Time: 0m 26s\n",
      "\tTrain Loss: 0.738 | Train PPL:   2.092\n",
      "\t Val. Loss: 13.811 |  Val. PPL: 995476.685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 154 | Time: 0m 26s\n",
      "\tTrain Loss: 0.768 | Train PPL:   2.156\n",
      "\t Val. Loss: 13.808 |  Val. PPL: 992087.554\n",
      "Epoch: 155 | Time: 0m 26s\n",
      "\tTrain Loss: 0.719 | Train PPL:   2.052\n",
      "\t Val. Loss: 13.892 |  Val. PPL: 1079464.241\n",
      "Epoch: 156 | Time: 0m 26s\n",
      "\tTrain Loss: 0.829 | Train PPL:   2.291\n",
      "\t Val. Loss: 13.890 |  Val. PPL: 1077852.286\n",
      "Epoch: 157 | Time: 0m 26s\n",
      "\tTrain Loss: 0.812 | Train PPL:   2.253\n",
      "\t Val. Loss: 13.941 |  Val. PPL: 1133952.963\n",
      "Epoch: 158 | Time: 0m 26s\n",
      "\tTrain Loss: 1.013 | Train PPL:   2.755\n",
      "\t Val. Loss: 13.855 |  Val. PPL: 1040430.840\n",
      "Epoch: 159 | Time: 0m 26s\n",
      "\tTrain Loss: 0.856 | Train PPL:   2.353\n",
      "\t Val. Loss: 13.925 |  Val. PPL: 1115376.578\n",
      "Epoch: 160 | Time: 0m 26s\n",
      "\tTrain Loss: 0.762 | Train PPL:   2.142\n",
      "\t Val. Loss: 13.871 |  Val. PPL: 1056807.179\n",
      "Epoch: 161 | Time: 0m 26s\n",
      "\tTrain Loss: 0.707 | Train PPL:   2.028\n",
      "\t Val. Loss: 14.191 |  Val. PPL: 1455588.479\n",
      "Epoch: 162 | Time: 0m 27s\n",
      "\tTrain Loss: 0.719 | Train PPL:   2.051\n",
      "\t Val. Loss: 14.107 |  Val. PPL: 1338009.029\n",
      "Epoch: 163 | Time: 0m 26s\n",
      "\tTrain Loss: 0.667 | Train PPL:   1.948\n",
      "\t Val. Loss: 14.345 |  Val. PPL: 1698831.730\n",
      "Epoch: 164 | Time: 0m 26s\n",
      "\tTrain Loss: 0.627 | Train PPL:   1.872\n",
      "\t Val. Loss: 14.330 |  Val. PPL: 1672852.702\n",
      "Epoch: 165 | Time: 0m 26s\n",
      "\tTrain Loss: 0.586 | Train PPL:   1.797\n",
      "\t Val. Loss: 14.485 |  Val. PPL: 1954092.481\n",
      "Epoch: 166 | Time: 0m 26s\n",
      "\tTrain Loss: 0.601 | Train PPL:   1.824\n",
      "\t Val. Loss: 14.434 |  Val. PPL: 1856725.886\n",
      "Epoch: 167 | Time: 0m 26s\n",
      "\tTrain Loss: 0.620 | Train PPL:   1.860\n",
      "\t Val. Loss: 14.536 |  Val. PPL: 2055435.580\n",
      "Epoch: 168 | Time: 0m 26s\n",
      "\tTrain Loss: 0.606 | Train PPL:   1.834\n",
      "\t Val. Loss: 14.409 |  Val. PPL: 1810711.160\n",
      "Epoch: 169 | Time: 0m 26s\n",
      "\tTrain Loss: 0.553 | Train PPL:   1.738\n",
      "\t Val. Loss: 14.798 |  Val. PPL: 2672122.645\n",
      "Epoch: 170 | Time: 0m 27s\n",
      "\tTrain Loss: 0.543 | Train PPL:   1.721\n",
      "\t Val. Loss: 14.905 |  Val. PPL: 2973373.230\n",
      "Epoch: 171 | Time: 0m 27s\n",
      "\tTrain Loss: 0.528 | Train PPL:   1.695\n",
      "\t Val. Loss: 14.798 |  Val. PPL: 2670920.102\n",
      "Epoch: 172 | Time: 0m 26s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.716\n",
      "\t Val. Loss: 14.835 |  Val. PPL: 2771404.021\n",
      "Epoch: 173 | Time: 0m 26s\n",
      "\tTrain Loss: 0.738 | Train PPL:   2.091\n",
      "\t Val. Loss: 14.563 |  Val. PPL: 2111725.487\n",
      "Epoch: 174 | Time: 0m 26s\n",
      "\tTrain Loss: 0.654 | Train PPL:   1.923\n",
      "\t Val. Loss: 14.823 |  Val. PPL: 2739382.646\n",
      "Epoch: 175 | Time: 0m 26s\n",
      "\tTrain Loss: 0.560 | Train PPL:   1.751\n",
      "\t Val. Loss: 14.858 |  Val. PPL: 2836698.506\n",
      "Epoch: 176 | Time: 0m 27s\n",
      "\tTrain Loss: 0.513 | Train PPL:   1.671\n",
      "\t Val. Loss: 15.192 |  Val. PPL: 3961086.837\n",
      "Epoch: 177 | Time: 0m 26s\n",
      "\tTrain Loss: 0.471 | Train PPL:   1.602\n",
      "\t Val. Loss: 15.175 |  Val. PPL: 3892746.068\n",
      "Epoch: 178 | Time: 0m 26s\n",
      "\tTrain Loss: 0.477 | Train PPL:   1.611\n",
      "\t Val. Loss: 15.301 |  Val. PPL: 4416762.967\n",
      "Epoch: 179 | Time: 0m 27s\n",
      "\tTrain Loss: 0.447 | Train PPL:   1.564\n",
      "\t Val. Loss: 15.347 |  Val. PPL: 4626022.673\n",
      "Epoch: 180 | Time: 0m 30s\n",
      "\tTrain Loss: 0.431 | Train PPL:   1.538\n",
      "\t Val. Loss: 15.434 |  Val. PPL: 5044173.791\n",
      "Epoch: 181 | Time: 0m 31s\n",
      "\tTrain Loss: 0.407 | Train PPL:   1.502\n",
      "\t Val. Loss: 15.320 |  Val. PPL: 4500355.099\n",
      "Epoch: 182 | Time: 0m 28s\n",
      "\tTrain Loss: 0.542 | Train PPL:   1.719\n",
      "\t Val. Loss: 15.360 |  Val. PPL: 4686247.473\n",
      "Epoch: 183 | Time: 0m 27s\n",
      "\tTrain Loss: 0.510 | Train PPL:   1.666\n",
      "\t Val. Loss: 15.557 |  Val. PPL: 5705504.600\n",
      "Epoch: 184 | Time: 0m 27s\n",
      "\tTrain Loss: 0.623 | Train PPL:   1.865\n",
      "\t Val. Loss: 15.484 |  Val. PPL: 5303917.927\n",
      "Epoch: 185 | Time: 0m 28s\n",
      "\tTrain Loss: 0.452 | Train PPL:   1.572\n",
      "\t Val. Loss: 15.623 |  Val. PPL: 6093650.660\n",
      "Epoch: 186 | Time: 0m 26s\n",
      "\tTrain Loss: 0.433 | Train PPL:   1.542\n",
      "\t Val. Loss: 15.296 |  Val. PPL: 4393577.164\n",
      "Epoch: 187 | Time: 0m 26s\n",
      "\tTrain Loss: 0.382 | Train PPL:   1.466\n",
      "\t Val. Loss: 15.526 |  Val. PPL: 5531868.842\n",
      "Epoch: 188 | Time: 0m 26s\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.445\n",
      "\t Val. Loss: 15.369 |  Val. PPL: 4729515.232\n",
      "Epoch: 189 | Time: 0m 26s\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.390\n",
      "\t Val. Loss: 15.628 |  Val. PPL: 6127788.866\n",
      "Epoch: 190 | Time: 0m 26s\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.368\n",
      "\t Val. Loss: 15.713 |  Val. PPL: 6668031.933\n",
      "Epoch: 191 | Time: 0m 26s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.411\n",
      "\t Val. Loss: 15.834 |  Val. PPL: 7529470.866\n",
      "Epoch: 192 | Time: 0m 26s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.406\n",
      "\t Val. Loss: 15.603 |  Val. PPL: 5976444.203\n",
      "Epoch: 193 | Time: 0m 26s\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.391\n",
      "\t Val. Loss: 15.879 |  Val. PPL: 7876690.396\n",
      "Epoch: 194 | Time: 0m 26s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 15.861 |  Val. PPL: 7733584.537\n",
      "Epoch: 195 | Time: 0m 29s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 16.019 |  Val. PPL: 9059593.352\n",
      "Epoch: 196 | Time: 0m 29s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 16.037 |  Val. PPL: 9219636.420\n",
      "Epoch: 197 | Time: 0m 30s\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
      "\t Val. Loss: 16.020 |  Val. PPL: 9067804.977\n",
      "Epoch: 198 | Time: 0m 28s\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.457\n",
      "\t Val. Loss: 16.001 |  Val. PPL: 8893401.542\n",
      "Epoch: 199 | Time: 0m 27s\n",
      "\tTrain Loss: 0.388 | Train PPL:   1.474\n",
      "\t Val. Loss: 16.072 |  Val. PPL: 9552516.791\n",
      "Epoch: 200 | Time: 0m 26s\n",
      "\tTrain Loss: 0.348 | Train PPL:   1.417\n",
      "\t Val. Loss: 15.855 |  Val. PPL: 7689972.320\n",
      "Epoch: 201 | Time: 0m 26s\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.349\n",
      "\t Val. Loss: 16.054 |  Val. PPL: 9380699.450\n",
      "Epoch: 202 | Time: 0m 26s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
      "\t Val. Loss: 16.191 |  Val. PPL: 10754676.181\n",
      "Epoch: 203 | Time: 0m 26s\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 16.401 |  Val. PPL: 13264506.487\n",
      "Epoch: 204 | Time: 0m 26s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 16.381 |  Val. PPL: 13004350.654\n",
      "Epoch: 205 | Time: 0m 28s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 16.468 |  Val. PPL: 14194737.875\n",
      "Epoch: 206 | Time: 0m 28s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 16.426 |  Val. PPL: 13610648.079\n",
      "Epoch: 207 | Time: 0m 27s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 16.489 |  Val. PPL: 14495010.329\n",
      "Epoch: 208 | Time: 0m 27s\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
      "\t Val. Loss: 16.406 |  Val. PPL: 13330246.557\n",
      "Epoch: 209 | Time: 0m 26s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 16.679 |  Val. PPL: 17521944.327\n",
      "Epoch: 210 | Time: 0m 27s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 16.493 |  Val. PPL: 14549410.942\n",
      "Epoch: 211 | Time: 0m 26s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 16.406 |  Val. PPL: 13340598.723\n",
      "Epoch: 212 | Time: 0m 27s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 16.439 |  Val. PPL: 13784593.570\n",
      "Epoch: 213 | Time: 0m 27s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 16.437 |  Val. PPL: 13759271.344\n",
      "Epoch: 214 | Time: 0m 27s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 16.381 |  Val. PPL: 13001820.910\n",
      "Epoch: 215 | Time: 0m 26s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 16.470 |  Val. PPL: 14211317.030\n",
      "Epoch: 216 | Time: 0m 27s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 16.506 |  Val. PPL: 14732627.943\n",
      "Epoch: 217 | Time: 0m 26s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 16.616 |  Val. PPL: 16453092.597\n",
      "Epoch: 218 | Time: 0m 26s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 16.838 |  Val. PPL: 20539297.713\n",
      "Epoch: 219 | Time: 0m 26s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 17.077 |  Val. PPL: 26099853.568\n",
      "Epoch: 220 | Time: 0m 26s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 16.874 |  Val. PPL: 21288553.987\n",
      "Epoch: 221 | Time: 0m 26s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 16.906 |  Val. PPL: 21997781.800\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1486], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_EPOCHS):\n\u001b[1;32m      8\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 10\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLIP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m evaluate(model, criterion)\n\u001b[1;32m     13\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[1483], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     10\u001b[0m trg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(trg)\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 14\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# flattening and getting rid of <sos> and 0 in trg and output respectively\u001b[39;00m\n\u001b[1;32m     18\u001b[0m output \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[1478], line 74\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     70\u001b[0m trg_vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39moutput_size\n\u001b[1;32m     72\u001b[0m outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(trg_len, batch_size, trg_vocab_size)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 74\u001b[0m hidden, cell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m#first input to the decoder is the <sos> tokens\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m trg[\u001b[38;5;241m0\u001b[39m,:]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[1478], line 18\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, i):\n\u001b[1;32m     17\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(i))\n\u001b[0;32m---> 18\u001b[0m     o, (h,c) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m)\u001b[49m      \n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m h, c\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 812\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    815\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    816\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 1000\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1508,
   "id": "2b4a1488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  ['when', 'did', 'beyonce', 'start', 'becoming', 'popular']\n",
      "Answer/Ground Truth:  ['in', 'the', 'late', '1990s']\n",
      "march EOS EOS EOS EOS EOS EOS\n"
     ]
    }
   ],
   "source": [
    "print(\"Question: \", a[0])\n",
    "print(\"Answer/Ground Truth: \",b[0])\n",
    "\n",
    "query = [vocab.word_2_index[word] for word in a[0]]\n",
    "query.insert(0, SOS_token) \n",
    "query.append(EOS_token)\n",
    "\n",
    "response = [0 for word in a[0]]\n",
    "response.insert(0, SOS_token) \n",
    "response.append(EOS_token)\n",
    "\n",
    "q_temp = np.array([query]).T\n",
    "src = torch.from_numpy(q_temp)\n",
    "r_temp = np.array([response]).T\n",
    "trg = torch.from_numpy(r_temp)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(src, trg, teacher_forcing_ratio=0)\n",
    "\n",
    "outputs.shape\n",
    "\n",
    "output_idx = outputs[1:].squeeze(1).argmax(1)\n",
    "l = output_idx.numpy().tolist()\n",
    "x = ' '.join([trg_vocab.index_2_word[idx] for idx in l])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1517,
   "id": "05994373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  ['where', 'did', 'beyonce', 'perform', 'in']\n",
      "Answer/Ground Truth:  ['glastonbury', 'festival']\n",
      "y adult contemporary EOS EOS EOS\n"
     ]
    }
   ],
   "source": [
    "print(\"Question: \", a[100])\n",
    "print(\"Answer/Ground Truth: \",b[100])\n",
    "\n",
    "query = [vocab.word_2_index[word] for word in a[100]]\n",
    "query.insert(0, SOS_token) \n",
    "query.append(EOS_token)\n",
    "\n",
    "response = [0 for word in a[100]]\n",
    "response.insert(0, SOS_token) \n",
    "response.append(EOS_token)\n",
    "\n",
    "q_temp = np.array([query]).T\n",
    "src = torch.from_numpy(q_temp)\n",
    "r_temp = np.array([response]).T\n",
    "trg = torch.from_numpy(r_temp)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(src, trg, teacher_forcing_ratio=0)\n",
    "\n",
    "outputs.shape\n",
    "\n",
    "output_idx = outputs[1:].squeeze(1).argmax(1)\n",
    "l = output_idx.numpy().tolist()\n",
    "x = ' '.join([trg_vocab.index_2_word[idx] for idx in l])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1518,
   "id": "c2b18038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  ['who', 'did', 'beyonc', 'perform', 'privately', 'for', 'in']\n",
      "Answer/Ground Truth:  ['muammar', 'gaddafi']\n",
      "new ivy campaign EOS EOS EOS EOS EOS\n"
     ]
    }
   ],
   "source": [
    "print(\"Question: \", a[101])\n",
    "print(\"Answer/Ground Truth: \",b[101])\n",
    "\n",
    "query = [vocab.word_2_index[word] for word in a[101]]\n",
    "query.insert(0, SOS_token) \n",
    "query.append(EOS_token)\n",
    "\n",
    "response = [0 for word in a[101]]\n",
    "response.insert(0, SOS_token) \n",
    "response.append(EOS_token)\n",
    "\n",
    "q_temp = np.array([query]).T\n",
    "src = torch.from_numpy(q_temp)\n",
    "r_temp = np.array([response]).T\n",
    "trg = torch.from_numpy(r_temp)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(src, trg, teacher_forcing_ratio=0)\n",
    "\n",
    "outputs.shape\n",
    "\n",
    "output_idx = outputs[1:].squeeze(1).argmax(1)\n",
    "l = output_idx.numpy().tolist()\n",
    "x = ' '.join([trg_vocab.index_2_word[idx] for idx in l])\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
